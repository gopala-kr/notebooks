{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 PyTorch CPU to GPU copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% reset -f\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 2.7.12 (default, Nov 19 2016, 06:48:10) \n",
      "[GCC 5.4.0 20160609]\n",
      "__pyTorch VERSION: 0.1.12+4eb448a\n",
      "__CUDA VERSION\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2016 NVIDIA Corporation\n",
      "Built on Tue_Jan_10_13:22:03_CST_2017\n",
      "Cuda compilation tools, release 8.0, V8.0.61\n",
      "__CUDNN VERSION: 5110\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alloocate a PyTorch Tensor on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n",
      "\n",
      "1.00000e-31 *\n",
      " -6.5516  0.0000  0.0000  0.0000\n",
      " -0.0008  0.0000 -0.0011  0.0000\n",
      " -0.0009  0.0000 -0.0011  0.0000\n",
      "[torch.cuda.FloatTensor of size 3x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor(3,4)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()*2     \n",
    "print (type(x))\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (2097152,), 16.0 MB\n",
      "from_numpy sent to GPU       0.002 seconds\n",
      "CPU constructor              0.054 seconds\n",
      "CPU constructor sent to GPU  0.054 seconds\n",
      "GPU constructor              0.056 seconds\n",
      "shape (128, 16384), 16.0 MB\n",
      "from_numpy sent to GPU       0.003 seconds\n",
      "CPU constructor              0.053 seconds\n",
      "CPU constructor sent to GPU  0.054 seconds\n",
      "GPU constructor              0.144 seconds\n",
      "shape (128, 128, 128), 16.0 MB\n",
      "from_numpy sent to GPU       0.003 seconds\n",
      "CPU constructor              0.055 seconds\n",
      "CPU constructor sent to GPU  0.056 seconds\n",
      "GPU constructor             12.634 seconds\n",
      "shape (32, 32, 32, 64), 16.0 MB\n",
      "from_numpy sent to GPU       0.003 seconds\n",
      "CPU constructor              0.057 seconds\n",
      "CPU constructor sent to GPU  0.057 seconds\n",
      "GPU constructor             25.269 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.cuda as cu\n",
    "import contextlib\n",
    "import time\n",
    "\n",
    "\n",
    "# allocates a tensor on GPU 1\n",
    "a = torch.cuda.FloatTensor(1)\n",
    "\n",
    "# transfers a tensor from CPU to GPU 1\n",
    "b = torch.FloatTensor(1).cuda()\n",
    "    \n",
    "# Timing helper with CUDA synchonization\n",
    "@contextlib.contextmanager\n",
    "def timing(name):\n",
    "    cu.synchronize()\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    cu.synchronize()\n",
    "    end_time = time.time()\n",
    "    print ('{} {:6.3f} seconds'.format(name, end_time-start_time))\n",
    "    \n",
    "    \n",
    "for shape in [(128**3,), (128,128**2), (128,128,128), (32,32,32,64)]:\n",
    "    print ('shape {}, {:.1f} MB'.format(shape, np.zeros(shape).nbytes/1024.**2))\n",
    "\n",
    "    with timing('from_numpy sent to GPU     '): torch.from_numpy (np.zeros(shape)).cuda()\n",
    "    with timing('CPU constructor            '): torch.FloatTensor(np.zeros(shape))\n",
    "    with timing('CPU constructor sent to GPU'): torch.FloatTensor(np.zeros(shape)).cuda()\n",
    "    with timing('GPU constructor            '): cu.   FloatTensor(np.zeros(shape))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Should be using **torch.from_numpy(x).cuda()**\n",
    "\n",
    "Refer to https://github.com/pytorch/pytorch/issues/1299  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}