{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Bootcamp November 2017, GPU Computing for Data Scientists\n",
    "\n",
    "<img src=\"../images/bcamp.png\" align=\"center\">\n",
    "\n",
    "## 05 PyTorch Automatic differentiation\n",
    "\n",
    "Web: https://www.meetup.com/Tel-Aviv-Deep-Learning-Bootcamp/events/241762893/\n",
    "\n",
    "Notebooks: <a href=\"https://github.com/QuantScientist/Data-Science-PyCUDA-GPU\"> On GitHub</a>\n",
    "\n",
    "*Shlomo Kashani*\n",
    "\n",
    "<img src=\"../images/pt.jpg\" width=\"35%\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.4.0\n",
      "('__Python VERSION:', '2.7.12 (default, Nov 19 2016, 06:48:10) \\n[GCC 5.4.0 20160609]')\n",
      "('__pyTorch VERSION:', '0.2.0_2')\n",
      "__CUDA VERSION\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2016 NVIDIA Corporation\n",
      "Built on Tue_Jan_10_13:22:03_CST_2017\n",
      "Cuda compilation tools, release 8.0, V8.0.61\n",
      "('__CUDNN VERSION:', 6021)\n",
      "('__Number CUDA Devices:', 1L)\n",
      "__Devices\n",
      "('Active CUDA Device: GPU', 0L)\n",
      "('Available devices ', 1L)\n",
      "('Current cuda device ', 0L)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pycuda\n",
    "%reset -f\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "# imports\n",
    "import numpy as np                     # numeric python lib\n",
    "import matplotlib.image as mpimg       # reading images to numpy arrays\n",
    "import matplotlib.pyplot as plt        # to plot any graph\n",
    "import matplotlib.patches as mpatches  # to draw a circle at the mean contour\n",
    "import scipy.ndimage as ndi            # to determine shape centrality\n",
    "# matplotlib setup\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots\n",
    "\n",
    "import tensorflow as tf \n",
    "print(\"tensorflow:\" + tf.__version__)\n",
    "!set \"KERAS_BACKEND=tensorflow\"\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pure Autograd\n",
    "\n",
    "Autograd is a Python package for automatic differentiation: github.com/HIPS/autograd\n",
    " \n",
    "- Autograd can automatically differentiate Python and Numpy code\n",
    "- It can handle most of Pythonâ€™s features, including loops, if statements, recursion and closures\n",
    "- It can also compute higher-order derivatives\n",
    "- Uses reverse-mode differentiation (backpropagation) so it can efficiently take gradients of scalar-valued functions with respect to array-valued or vector-valued arguments.\n",
    "- derivatives of derivatives\n",
    "\n",
    "### Autograd is installed by running the following command in the terminal:\n",
    "\n",
    " *`pip install autograd`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single variable example\n",
    "\n",
    "There are a couple of things to note from this example. \n",
    "autograd.numpy is a thinly- wrapped NumPy. Also, *grad()* returns a function that computes the gradient of your original function. This new function which returns the gradient accepts the same parameters as the original function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.393223866483\n"
     ]
    }
   ],
   "source": [
    "# Thinly wrapped numpy\n",
    "import autograd.numpy as np\n",
    "# Basically everything you need\n",
    "from autograd import grad\n",
    "\n",
    "# Define a function like normal with Python and Numpy\n",
    "def tanh(x):\n",
    "    y = np.exp(-x)\n",
    "    return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "# Create a function to compute the gradient\n",
    "grad_tanh = grad(tanh)\n",
    "# Evaluate the gradient at x = 1.0\n",
    "print(grad_tanh(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple variables example\n",
    "\n",
    "When there are multiple variables, the parameter *argnum* allows you to specify with respect to which variable you are computing the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3125\n",
      "2.75\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name multigrad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6f7a66e35181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Finding the gradient with respect to multiple variables can by done using multigrad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# by specifying which variables in the argnums parameter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultigrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgrad_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultigrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name multigrad"
     ]
    }
   ],
   "source": [
    "f = lambda x,y: 3*x*y + 2*y - x**3\n",
    "grad_f = grad(f, argnum=0) #gradient with respect to the first variable\n",
    "print grad_f(.25,.5)\n",
    "grad_f = grad(f, argnum=1) #gradient with respect to the second variable\n",
    "print grad_f(.25,.5)\n",
    "\n",
    "# Finding the gradient with respect to multiple variables can by done using multigrad() \n",
    "# by specifying which variables in the argnums parameter.\n",
    "from autograd import multigrad\n",
    "grad_fun = multigrad(grad_f, argnums=[0,1])\n",
    "print grad_fun(.25,.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "livereveal": {
   "controls": "true",
   "history": "true",
   "mouseWheel": "true",
   "overview": "true",
   "progress": "true",
   "scroll": "true",
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
