{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (61878, 95) \r\n",
      "test shape is: (144368, 94) \n"
     ]
    }
   ],
   "source": [
    "tr_data = pd.read_csv('../input/train.csv')\n",
    "te_data = pd.read_csv('../input/test.csv')\n",
    "print('train shape is: {} \\r\\ntest shape is: {} '.format(tr_data.shape,te_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "y = tr_data.target\n",
    "tr_ids = tr_data.id\n",
    "te_ids = te_data.id\n",
    "tr_data.drop(['id','target'],axis=1,inplace=True)\n",
    "te_data.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_data['count_zeros'] = (tr_data == 0).astype(int).sum(axis=1)\n",
    "te_data['count_zeros'] = (te_data == 0).astype(int).sum(axis=1)\n",
    "\n",
    "tr_data['num_greater_than_3'] = (tr_data > 3).astype(int).sum(axis=1)\n",
    "te_data['num_greater_than_3'] = (te_data > 3).astype(int).sum(axis=1)\n",
    "\n",
    "tr_data['num_greater_than_10'] = (tr_data > 10).astype(int).sum(axis=1)\n",
    "te_data['num_greater_than_10'] = (te_data > 10).astype(int).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tr_data_tfidf = pd.DataFrame(tfidf.fit_transform(tr_data).toarray())\n",
    "te_data_tfidf = pd.DataFrame(tfidf.transform(te_data).toarray())\n",
    "tr_data_tfidf.columns = [str(x)+'tfidf' for x in tr_data_tfidf.columns]\n",
    "te_data_tfidf.columns = [str(x)+'tfidf' for x in te_data_tfidf.columns]\n",
    "\n",
    "tr_data_log1p = pd.DataFrame(tr_data.apply(lambda x: np.log1p(x)))\n",
    "te_data_log1p = pd.DataFrame(te_data.apply(lambda x: np.log1p(x)))\n",
    "tr_data_log1p.columns = [str(x)+'log1p' for x in tr_data_log1p.columns]\n",
    "te_data_log1p.columns = [str(x)+'log1p' for x in te_data_log1p.columns]\n",
    "\n",
    "tr_data_comb = pd.concat([tr_data,tr_data_tfidf,tr_data_log1p],axis=1)\n",
    "te_data_comb = pd.concat([te_data,te_data_tfidf,te_data_log1p],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# y_encoded = le.fit_transform(y)\n",
    "y_encoded = [int(x.split('_')[1]) for x in y]\n",
    "y_encoded = [y-1 for y in y_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix,log_loss\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(tr_data_comb,y_encoded,test_size = 0.2,random_state =12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.9474+0.00072084\ttest-mlogloss:1.955+0.000620196\n",
      "[2]\ttrain-mlogloss:1.62157+0.000899508\ttest-mlogloss:1.641+0.00145632\n",
      "[4]\ttrain-mlogloss:1.40371+0.0012475\ttest-mlogloss:1.43284+0.00202743\n",
      "[6]\ttrain-mlogloss:1.24356+0.00175519\ttest-mlogloss:1.28098+0.00266732\n",
      "[8]\ttrain-mlogloss:1.11843+0.00158386\ttest-mlogloss:1.16309+0.00315141\n",
      "[10]\ttrain-mlogloss:1.01842+0.0016977\ttest-mlogloss:1.06957+0.00338205\n",
      "[12]\ttrain-mlogloss:0.936599+0.0017721\ttest-mlogloss:0.993884+0.0036907\n",
      "[14]\ttrain-mlogloss:0.868352+0.00185664\ttest-mlogloss:0.931137+0.00362796\n",
      "[16]\ttrain-mlogloss:0.811162+0.00168662\ttest-mlogloss:0.87921+0.00341374\n",
      "[18]\ttrain-mlogloss:0.761766+0.00178435\ttest-mlogloss:0.835099+0.00350619\n",
      "[20]\ttrain-mlogloss:0.719773+0.00195614\ttest-mlogloss:0.798319+0.00367584\n",
      "[22]\ttrain-mlogloss:0.682774+0.00211222\ttest-mlogloss:0.766007+0.00364953\n",
      "[24]\ttrain-mlogloss:0.650396+0.0019471\ttest-mlogloss:0.738084+0.00364314\n",
      "[26]\ttrain-mlogloss:0.622625+0.00185526\ttest-mlogloss:0.714523+0.00374394\n",
      "[28]\ttrain-mlogloss:0.597821+0.00191458\ttest-mlogloss:0.693971+0.00379583\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {'objective':'multi:softprob',\n",
    "                    'learning_rate':0.1,\n",
    "                    'subsample':0.8,\n",
    "                    'colsample_bytree':0.9,\n",
    "                    'colsample_bylevel':0.7,\n",
    "                    'max_depth':7,\n",
    "                    'nthread':4,\n",
    "                    'eval_metric':'mlogloss',\n",
    "                    'num_class':9,\n",
    "                    'gamma':0.1,\n",
    "                    'seed':1234}\n",
    "\n",
    "bst_cv = xgb.cv(params=params,dtrain=xgb.DMatrix(tr_data_comb,label=y_encoded),verbose_eval=2,\n",
    "                nfold=5,early_stopping_rounds=20,num_boost_round=30)\n",
    "#try 400 boosting rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bst = xgb.train(params=params,dtrain=xgb.DMatrix(tr_data_comb,label=y_encoded),num_boost_round=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = bst.predict(xgb.DMatrix(te_data_comb))\n",
    "subm = pd.DataFrame(pred)\n",
    "subm.columns = ['class_'+ str(x) for x in range(1,10)]\n",
    "subm['id'] = pd.read_csv('../input/test.csv',usecols=['id'])\n",
    "#subm.index_label = 'id'\n",
    "subm.to_csv('../subm/tfidf_log1p_raw_xgb_sub1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's ignore the tfidf transform for and see what you could also come up with based on the things we learned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for knn2: 4.8441469936134425\n",
      "log loss for knn4: 2.6644552310076475\n",
      "log loss for knn8: 1.5318682781966835\n",
      "log loss for knn16: 0.9965564078189585\n",
      "log loss for knn32: 0.7943305611598044\n",
      "log loss for knn64: 0.7002169581175647\n",
      "log loss for knn128: 0.6943451757833374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix,log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(tr_data,y_encoded,test_size = 0.2,random_state =12345)\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_jobs=4,n_neighbors=2,)\n",
    "knn2.fit(X_train,y_train)\n",
    "knn2_pred = knn2.predict_proba(X_val)\n",
    "print('log loss for knn2: {}'.format(log_loss(y_pred = knn2_pred,y_true = y_val)))\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_jobs=4,n_neighbors=4,)\n",
    "knn4.fit(X_train,y_train)\n",
    "knn4_pred = knn4.predict_proba(X_val)\n",
    "print('log loss for knn4: {}'.format(log_loss(y_pred = knn4_pred,y_true = y_val)))\n",
    "\n",
    "knn8 = KNeighborsClassifier(n_jobs=8,n_neighbors=8,)\n",
    "knn8.fit(X_train,y_train)\n",
    "knn8_pred = knn8.predict_proba(X_val)\n",
    "print('log loss for knn8: {}'.format(log_loss(y_pred = knn8_pred,y_true = y_val)))\n",
    "\n",
    "knn16 = KNeighborsClassifier(n_jobs=4,n_neighbors=16,)\n",
    "knn16.fit(X_train,y_train)\n",
    "knn16_pred = knn16.predict_proba(X_val)\n",
    "print('log loss for knn16: {}'.format(log_loss(y_pred = knn16_pred,y_true = y_val)))\n",
    "\n",
    "knn32 = KNeighborsClassifier(n_jobs=4,n_neighbors=32,)\n",
    "knn32.fit(X_train,y_train)\n",
    "knn32_pred = knn32.predict_proba(X_val)\n",
    "print('log loss for knn32: {}'.format(log_loss(y_pred = knn32_pred,y_true = y_val)))\n",
    "\n",
    "knn64 = KNeighborsClassifier(n_jobs=4,n_neighbors=64,)\n",
    "knn64.fit(X_train,y_train)\n",
    "knn64_pred = knn64.predict_proba(X_val)\n",
    "print('log loss for knn64: {}'.format(log_loss(y_pred = knn64_pred,y_true = y_val)))\n",
    "\n",
    "knn128 = KNeighborsClassifier(n_jobs=4,n_neighbors=128,)\n",
    "knn128.fit(X_train,y_train)\n",
    "knn128_pred = knn128.predict_proba(X_val)\n",
    "print('log loss for knn128: {}'.format(log_loss(y_pred = knn128_pred,y_true = y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for dtc: 1.647334289958738\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:1,1:1,2:1,3:10,4:1,5:1,6:1,7:1,8:1}\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(class_weight=class_weights,max_depth=4,max_features=92,min_samples_split=2,random_state=12345)\n",
    "dtc.fit(X_train,y_train)\n",
    "tree_pred = dtc.predict_proba(X_val)\n",
    "print('log loss for dtc: {}'.format(log_loss(y_pred = tree_pred,y_true = y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for dtc: 1.4348667288258288\n"
     ]
    }
   ],
   "source": [
    "# class_weights = {0:1,1:1,2:1,3:1,4:1,5:1,6:10,7:1,8:1}\n",
    "# lets remove the class weights and check our score...\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=4,max_features=92,min_samples_split=2,random_state=12345)\n",
    "dtc.fit(X_train,y_train)\n",
    "tree_pred = dtc.predict_proba(X_val)\n",
    "print('log loss for dtc: {}'.format(log_loss(y_pred = tree_pred,y_true = y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nati/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for svc: 0.6653540066424208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear',C=0.1,max_iter=10000,random_state=12345,probability=True)\n",
    "svc.fit(X_train,y_train)\n",
    "svc_pred = svc.predict_proba(X_val)\n",
    "print('log loss for svc: {}'.format(log_loss(y_pred = svc_pred,y_true = y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf',C=0.1,max_iter=10000,random_state=12345,probability=True,)\n",
    "svc.fit(X_train,y_train)\n",
    "svc_pred = svc.predict_proba(X_val)\n",
    "print('log loss for svc: {}'.format(log_loss(y_pred = svc_pred,y_true = y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for RFC: 0.5726116665922214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=4,n_estimators=300)\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc_pred = rfc.predict_proba(X_val)\n",
    "print('log loss for RFC: {}'.format(log_loss(y_pred = rfc_pred,y_true = y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_test_pred = rfc.predict_proba(te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:2.0774\ttrain-mlogloss:2.075\n",
      "Multiple eval metrics have been passed: 'train-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-mlogloss hasn't improved in 80 rounds.\n",
      "[50]\teval-mlogloss:0.765823\ttrain-mlogloss:0.704729\n",
      "[100]\teval-mlogloss:0.605446\ttrain-mlogloss:0.512647\n",
      "[150]\teval-mlogloss:0.553927\ttrain-mlogloss:0.438171\n",
      "[200]\teval-mlogloss:0.528086\ttrain-mlogloss:0.392941\n",
      "[250]\teval-mlogloss:0.512905\ttrain-mlogloss:0.359338\n",
      "[300]\teval-mlogloss:0.50243\ttrain-mlogloss:0.332146\n",
      "[350]\teval-mlogloss:0.49479\ttrain-mlogloss:0.309143\n",
      "[400]\teval-mlogloss:0.48858\ttrain-mlogloss:0.288172\n",
      "[450]\teval-mlogloss:0.484047\ttrain-mlogloss:0.27052\n",
      "[500]\teval-mlogloss:0.480213\ttrain-mlogloss:0.25378\n",
      "[550]\teval-mlogloss:0.477109\ttrain-mlogloss:0.238802\n",
      "[600]\teval-mlogloss:0.474876\ttrain-mlogloss:0.225237\n",
      "[650]\teval-mlogloss:0.473521\ttrain-mlogloss:0.212742\n",
      "[700]\teval-mlogloss:0.472207\ttrain-mlogloss:0.200934\n",
      "[750]\teval-mlogloss:0.471159\ttrain-mlogloss:0.189759\n",
      "[800]\teval-mlogloss:0.470579\ttrain-mlogloss:0.17962\n",
      "[850]\teval-mlogloss:0.470158\ttrain-mlogloss:0.170171\n",
      "[900]\teval-mlogloss:0.470253\ttrain-mlogloss:0.161341\n",
      "[950]\teval-mlogloss:0.470208\ttrain-mlogloss:0.153517\n",
      "[1000]\teval-mlogloss:0.470167\ttrain-mlogloss:0.146116\n",
      "[1050]\teval-mlogloss:0.470499\ttrain-mlogloss:0.139369\n",
      "[1100]\teval-mlogloss:0.471161\ttrain-mlogloss:0.132695\n",
      "[1150]\teval-mlogloss:0.472029\ttrain-mlogloss:0.126684\n",
      "[1200]\teval-mlogloss:0.472778\ttrain-mlogloss:0.120981\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-8a8bea751d6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m bst = xgb.train(params=params,dtrain=xgb.DMatrix(X_train,label=y_train),num_boost_round=3000,early_stopping_rounds=80,\n\u001b[0;32m---> 19\u001b[0;31m                 evals=watchlist,verbose_eval=50)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
    "dval=xgb.DMatrix(X_val,label=y_val)\n",
    "watchlist = [(dtrain, 'train'),(dval, 'eval')]\n",
    "\n",
    "params = {'objective':'multi:softprob',\n",
    "                    'learning_rate':0.05,\n",
    "                    'subsample':0.7,\n",
    "                    'colsample_bytree':0.8,\n",
    "                    'colsample_bylevel':0.7,\n",
    "                    'max_depth':6,\n",
    "                    'nthread':3,\n",
    "                    'eval_metric':'mlogloss',\n",
    "                    'num_class':9,\n",
    "                    'silent':0,\n",
    "                    'seed':1234}\n",
    "\n",
    "bst = xgb.train(params=params,dtrain=xgb.DMatrix(X_train,label=y_train),num_boost_round=3000,early_stopping_rounds=80,\n",
    "                evals=watchlist,verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for XGB: 0.46402587516456023\n"
     ]
    }
   ],
   "source": [
    "xgb_pred = bst.predict(xgb.DMatrix(X_val))\n",
    "print('log loss for XGB: {}'.format(log_loss(y_pred = xgb_pred,y_true = y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_test_pred = bst.predict(xgb.DMatrix(te_data))\n",
    "combined_pred = xgb_test_pred *0.85 + rfc_test_pred *0.15\n",
    "subm = pd.DataFrame(combined_pred)\n",
    "subm.columns = ['class_'+ str(x) for x in range(1,10)]\n",
    "subm['id'] = pd.read_csv('../input/test.csv',usecols=['id'])\n",
    "#subm.index_label = 'id'\n",
    "subm.to_csv('../subm/rf_xgb_sub1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Dropout,Merge,Embedding,BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "OHE_y_train = np_utils.to_categorical(y_train)\n",
    "OHE_y_val = np_utils.to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "cb = ReduceLROnPlateau(patience=0,factor=0.1,epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49502 samples, validate on 12376 samples\n",
      "Epoch 1/10\n",
      "49502/49502 [==============================] - 13s - loss: 0.7291 - val_loss: 0.5988\n",
      "Epoch 2/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.5849 - val_loss: 0.5516\n",
      "Epoch 3/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.5515 - val_loss: 0.5489\n",
      "Epoch 4/10\n",
      "49502/49502 [==============================] - 13s - loss: 0.5268 - val_loss: 0.5249\n",
      "Epoch 5/10\n",
      "49502/49502 [==============================] - 13s - loss: 0.5021 - val_loss: 0.4952\n",
      "Epoch 6/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.4830 - val_loss: 0.5169\n",
      "Epoch 7/10\n",
      "49502/49502 [==============================] - 13s - loss: 0.4676 - val_loss: 0.5033\n",
      "Epoch 8/10\n",
      "49502/49502 [==============================] - 13s - loss: 0.4069 - val_loss: 0.4754\n",
      "Epoch 9/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.3882 - val_loss: 0.4738\n",
      "Epoch 10/10\n",
      "49502/49502 [==============================] - 13s - loss: 0.3869 - val_loss: 0.4733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0f80139b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(500,input_shape=(X_train.shape[1],),activation='relu'))\n",
    "model1.add(Dropout(0.1))\n",
    "model1.add(Dense(250,activation='relu'))\n",
    "model1.add(Dropout(0.05))\n",
    "model1.add(Dense(50,activation='relu'))\n",
    "model1.add(Dense(9,activation='softmax'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "model1.fit(X_train.values,OHE_y_train,validation_data=[X_val.values,OHE_y_val],callbacks=[cb],epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12128/12376 [============================>.] - ETA: 0s[[ 225   11    1    0    1   26   13   41   92]\n",
      " [   2 2630  489   53    5    5   19   11    8]\n",
      " [   0  638  832   36    1    1   32    9    1]\n",
      " [   1  154   96  268    7   14   14    3    0]\n",
      " [   1    8    2    0  523    0    0    1    0]\n",
      " [  13   10    3    6    0 2699   34   25   20]\n",
      " [   8   46   41    5    0   27  392   17    2]\n",
      " [  20   10    3    0    2   37   21 1575   27]\n",
      " [  35   16    2    3    0   27    3   31  942]]\n",
      "classification report results:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.55      0.63       410\n",
      "          1       0.75      0.82      0.78      3222\n",
      "          2       0.57      0.54      0.55      1550\n",
      "          3       0.72      0.48      0.58       557\n",
      "          4       0.97      0.98      0.97       535\n",
      "          5       0.95      0.96      0.96      2810\n",
      "          6       0.74      0.73      0.74       538\n",
      "          7       0.92      0.93      0.92      1695\n",
      "          8       0.86      0.89      0.88      1059\n",
      "\n",
      "avg / total       0.81      0.81      0.81     12376\n",
      "\n",
      "log-loss for classifier: 0.473291540907604\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD8CAYAAACrbmW5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0f8434cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ann_pred = model1.predict_classes(X_val.values)\n",
    "from sklearn.metrics import classification_report,log_loss\n",
    "print(confusion_matrix(y_pred=ann_pred,y_true=y_val))\n",
    "sns.heatmap(confusion_matrix(y_pred=ann_pred+1,y_true=y_val),cmap='Greens',xticklabels=range(1,10),yticklabels=range(1,10))\n",
    "print('classification report results:\\r\\n' + classification_report(y_pred=ann_pred,y_true=y_val))\n",
    "print('log-loss for classifier: {}'.format(log_loss(y_pred=model1.predict(X_val.values),y_true=y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49502 samples, validate on 12376 samples\n",
      "Epoch 1/10\n",
      "49502/49502 [==============================] - 13s - loss: 0.7567 - val_loss: 0.5862\n",
      "Epoch 2/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.6026 - val_loss: 0.5620\n",
      "Epoch 3/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.5685 - val_loss: 0.5448\n",
      "Epoch 4/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.5448 - val_loss: 0.5256\n",
      "Epoch 5/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.5254 - val_loss: 0.5223\n",
      "Epoch 6/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.5086 - val_loss: 0.5086\n",
      "Epoch 7/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.4541 - val_loss: 0.4772\n",
      "Epoch 8/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.4406 - val_loss: 0.4725\n",
      "Epoch 9/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.4309 - val_loss: 0.4717\n",
      "Epoch 10/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.4220 - val_loss: 0.4705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0f05ffa20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(500,input_shape=(X_train.shape[1],),activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(250,activation='relu'))\n",
    "model1.add(Dropout(0.1))\n",
    "model1.add(Dense(50,activation='relu'))\n",
    "model1.add(Dense(9,activation='softmax'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "model1.fit(X_train.values,OHE_y_train,validation_data=[X_val.values,OHE_y_val],callbacks=[cb],epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49502 samples, validate on 12376 samples\n",
      "Epoch 1/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.7046 - val_loss: 0.6144\n",
      "Epoch 2/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.6118 - val_loss: 0.5676\n",
      "Epoch 3/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.5856 - val_loss: 0.5570\n",
      "Epoch 4/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.5168 - val_loss: 0.4955\n",
      "Epoch 5/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4948 - val_loss: 0.4893\n",
      "Epoch 6/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4789 - val_loss: 0.4857\n",
      "Epoch 7/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4764 - val_loss: 0.4858\n",
      "Epoch 8/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4767 - val_loss: 0.4857\n",
      "Epoch 9/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4740 - val_loss: 0.4856\n",
      "Epoch 10/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4744 - val_loss: 0.4858\n",
      "Train on 49502 samples, validate on 12376 samples\n",
      "Epoch 1/10\n",
      "49502/49502 [==============================] - 18s - loss: 0.7109 - val_loss: 0.5925\n",
      "Epoch 2/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.6170 - val_loss: 0.5632\n",
      "Epoch 3/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.5869 - val_loss: 0.5321\n",
      "Epoch 4/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.5658 - val_loss: 0.5492\n",
      "Epoch 5/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.5048 - val_loss: 0.4809\n",
      "Epoch 6/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4809 - val_loss: 0.4743\n",
      "Epoch 7/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4679 - val_loss: 0.4725\n",
      "Epoch 8/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4676 - val_loss: 0.4726\n",
      "Epoch 9/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4643 - val_loss: 0.4721\n",
      "Epoch 10/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4639 - val_loss: 0.4717\n",
      "Train on 49502 samples, validate on 12376 samples\n",
      "Epoch 1/10\n",
      "49502/49502 [==============================] - 18s - loss: 0.7003 - val_loss: 0.6351\n",
      "Epoch 2/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.6078 - val_loss: 0.5754\n",
      "Epoch 3/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.5814 - val_loss: 0.6000\n",
      "Epoch 4/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.5159 - val_loss: 0.5191\n",
      "Epoch 5/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4909 - val_loss: 0.5089\n",
      "Epoch 6/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4736 - val_loss: 0.5070\n",
      "Epoch 7/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4712 - val_loss: 0.5066\n",
      "Epoch 8/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4710 - val_loss: 0.5066\n",
      "Epoch 9/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4705 - val_loss: 0.5064\n",
      "Epoch 10/10\n",
      "49502/49502 [==============================] - 17s - loss: 0.4718 - val_loss: 0.5068\n",
      "Train on 49503 samples, validate on 12375 samples\n",
      "Epoch 1/10\n",
      "49503/49503 [==============================] - 18s - loss: 0.7023 - val_loss: 0.6303\n",
      "Epoch 2/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.6097 - val_loss: 0.5746\n",
      "Epoch 3/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.5783 - val_loss: 0.6150\n",
      "Epoch 4/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.5099 - val_loss: 0.5114\n",
      "Epoch 5/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4859 - val_loss: 0.5045\n",
      "Epoch 6/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4717 - val_loss: 0.5011\n",
      "Epoch 7/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4680 - val_loss: 0.5007\n",
      "Epoch 8/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4669 - val_loss: 0.5007\n",
      "Epoch 9/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4684 - val_loss: 0.5008\n",
      "Epoch 10/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4665 - val_loss: 0.5010\n",
      "Train on 49503 samples, validate on 12375 samples\n",
      "Epoch 1/10\n",
      "49503/49503 [==============================] - 17s - loss: 0.7085 - val_loss: 0.6496\n",
      "Epoch 2/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.6086 - val_loss: 0.5826\n",
      "Epoch 3/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.5789 - val_loss: 0.5797\n",
      "Epoch 4/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.5122 - val_loss: 0.5093\n",
      "Epoch 5/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4890 - val_loss: 0.5047\n",
      "Epoch 6/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4709 - val_loss: 0.4969\n",
      "Epoch 7/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4682 - val_loss: 0.4969\n",
      "Epoch 8/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4680 - val_loss: 0.4966\n",
      "Epoch 9/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4691 - val_loss: 0.4965\n",
      "Epoch 10/10\n",
      "49503/49503 [==============================] - 16s - loss: 0.4700 - val_loss: 0.4958\n"
     ]
    }
   ],
   "source": [
    "oof_preds = []\n",
    "test_preds = []\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=12345)\n",
    "for train_index, val_index in kf.split(X=tr_data_comb,y=y_encoded):\n",
    "    #print(train_index,val_index)\n",
    "    X_train, X_val = tr_data_comb.iloc[train_index,:], tr_data_comb.iloc[val_index,:]\n",
    "    y_train, y_val = np.array(y_encoded)[train_index], np.array(y_encoded)[val_index]\n",
    "    OHE_y_train = np_utils.to_categorical(y_train)\n",
    "    OHE_y_val = np_utils.to_categorical(y_val)\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(1500,input_shape=(X_train.shape[1],),activation='relu'))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Dropout(0.3))\n",
    "    model1.add(Dense(500,activation='relu'))\n",
    "    model1.add(Dropout(0.1))\n",
    "    model1.add(Dense(250,activation='relu'))\n",
    "    model1.add(Dense(9,activation='softmax'))\n",
    "    model1.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy')\n",
    "\n",
    "    model1.fit(X_train.values,OHE_y_train,validation_data=[X_val.values,OHE_y_val],callbacks=[cb],epochs=10)\n",
    "    \n",
    "    ann_pred = model1.predict(X_val.values)\n",
    "    ann_test_pred = model1.predict(te_data_comb.values)\n",
    "    oof_preds.append(ann_pred)\n",
    "    test_preds.append(ann_test_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_arr = np.array(test_preds)\n",
    "preds = pd.DataFrame(np.clip(np.mean(test_pred_arr,axis=0),a_max=0.999,a_min=0.001))\n",
    "subm = pd.DataFrame(preds)\n",
    "subm.columns = ['Class_'+ str(x) for x in range(1,10)]\n",
    "subm['id'] = te_ids\n",
    "subm.to_csv('../subm/ANN_5fold.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_2</th>\n",
       "      <th>class_3</th>\n",
       "      <th>class_4</th>\n",
       "      <th>class_5</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.208968</td>\n",
       "      <td>0.240721</td>\n",
       "      <td>0.547515</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.674757</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.321769</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.609515</td>\n",
       "      <td>0.388631</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069937</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.928398</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.312959</td>\n",
       "      <td>0.684896</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.963565</td>\n",
       "      <td>0.028423</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.427492</td>\n",
       "      <td>0.424492</td>\n",
       "      <td>0.124076</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.018744</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.428688</td>\n",
       "      <td>0.262573</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>0.215042</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006408</td>\n",
       "      <td>0.355690</td>\n",
       "      <td>0.389999</td>\n",
       "      <td>0.174736</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.068765</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.057557</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.939708</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.966769</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.855406</td>\n",
       "      <td>0.117514</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.062670</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>0.782979</td>\n",
       "      <td>0.137492</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.022623</td>\n",
       "      <td>0.976627</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.779807</td>\n",
       "      <td>0.181688</td>\n",
       "      <td>0.035555</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.476985</td>\n",
       "      <td>0.228144</td>\n",
       "      <td>0.242141</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.007388</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.034144</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.007656</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.018217</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.958579</td>\n",
       "      <td>0.015429</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.112242</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.067750</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.808263</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.145573</td>\n",
       "      <td>0.827702</td>\n",
       "      <td>0.026218</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.925317</td>\n",
       "      <td>0.066953</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.450735</td>\n",
       "      <td>0.538647</td>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.180890</td>\n",
       "      <td>0.819059</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144338</th>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.260690</td>\n",
       "      <td>0.406058</td>\n",
       "      <td>0.128609</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.006966</td>\n",
       "      <td>0.184826</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>144339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144339</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144340</th>\n",
       "      <td>0.032840</td>\n",
       "      <td>0.347416</td>\n",
       "      <td>0.267714</td>\n",
       "      <td>0.085864</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.040524</td>\n",
       "      <td>0.147880</td>\n",
       "      <td>0.047727</td>\n",
       "      <td>0.020626</td>\n",
       "      <td>144341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144341</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.809849</td>\n",
       "      <td>0.142712</td>\n",
       "      <td>0.047162</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144342</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144343</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144344</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.776325</td>\n",
       "      <td>0.194695</td>\n",
       "      <td>0.028288</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144345</th>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.988525</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>144346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144346</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144347</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.336146</td>\n",
       "      <td>0.525665</td>\n",
       "      <td>0.132012</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144348</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.827664</td>\n",
       "      <td>0.100055</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>144349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144349</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.572332</td>\n",
       "      <td>0.397430</td>\n",
       "      <td>0.028070</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144350</th>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.997659</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144351</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.664792</td>\n",
       "      <td>0.293183</td>\n",
       "      <td>0.042009</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144352</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.903193</td>\n",
       "      <td>0.092301</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144353</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.044817</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.954635</td>\n",
       "      <td>144354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144354</th>\n",
       "      <td>0.072238</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.737002</td>\n",
       "      <td>0.162776</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.023210</td>\n",
       "      <td>144355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144355</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.094825</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.905166</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144356</th>\n",
       "      <td>0.442786</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144357</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144358</th>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.966677</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>144359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144359</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.872030</td>\n",
       "      <td>0.096009</td>\n",
       "      <td>0.026228</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>144360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144360</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.038284</td>\n",
       "      <td>0.961553</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144361</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.531110</td>\n",
       "      <td>0.413039</td>\n",
       "      <td>0.054133</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144362</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.357569</td>\n",
       "      <td>0.509527</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.105559</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144363</th>\n",
       "      <td>0.384997</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.557151</td>\n",
       "      <td>0.021771</td>\n",
       "      <td>0.010221</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>144364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144364</th>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.325014</td>\n",
       "      <td>0.496746</td>\n",
       "      <td>0.136171</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.040175</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144365</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.671773</td>\n",
       "      <td>0.312076</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144366</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.466384</td>\n",
       "      <td>0.059653</td>\n",
       "      <td>0.473535</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144367</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.382231</td>\n",
       "      <td>0.519363</td>\n",
       "      <td>0.058214</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>144368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144368 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         class_1   class_2   class_3   class_4   class_5   class_6   class_7  \\\n",
       "0       0.001000  0.208968  0.240721  0.547515  0.001000  0.001000  0.002681   \n",
       "1       0.001000  0.001000  0.001000  0.001000  0.001000  0.674757  0.001000   \n",
       "2       0.001000  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000   \n",
       "3       0.001000  0.609515  0.388631  0.001851  0.001000  0.001000  0.001000   \n",
       "4       0.069937  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000   \n",
       "5       0.001000  0.312959  0.684896  0.001000  0.001000  0.001000  0.001000   \n",
       "6       0.001000  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000   \n",
       "7       0.001000  0.963565  0.028423  0.007456  0.001000  0.001000  0.001000   \n",
       "8       0.003016  0.427492  0.424492  0.124076  0.001000  0.001000  0.018744   \n",
       "9       0.005035  0.428688  0.262573  0.002361  0.010959  0.051200  0.017695   \n",
       "10      0.006408  0.355690  0.389999  0.174736  0.001000  0.001872  0.068765   \n",
       "11      0.057557  0.001000  0.001000  0.001000  0.001000  0.002416  0.001000   \n",
       "12      0.001000  0.001000  0.001000  0.001000  0.999000  0.001000  0.001000   \n",
       "13      0.001000  0.966769  0.024264  0.007309  0.001000  0.001000  0.001000   \n",
       "14      0.003499  0.855406  0.117514  0.009208  0.001000  0.001000  0.003543   \n",
       "15      0.062670  0.001000  0.001000  0.001000  0.001000  0.016578  0.782979   \n",
       "16      0.001000  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000   \n",
       "17      0.001000  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000   \n",
       "18      0.001000  0.001000  0.001000  0.022623  0.976627  0.001000  0.001000   \n",
       "19      0.001000  0.779807  0.181688  0.035555  0.001000  0.001000  0.002030   \n",
       "20      0.001000  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000   \n",
       "21      0.001000  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000   \n",
       "22      0.006439  0.476985  0.228144  0.242141  0.001000  0.002819  0.007388   \n",
       "23      0.007656  0.001000  0.001000  0.001000  0.001000  0.018217  0.001000   \n",
       "24      0.112242  0.001000  0.001000  0.001000  0.001000  0.067750  0.001000   \n",
       "25      0.001000  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000   \n",
       "26      0.001000  0.145573  0.827702  0.026218  0.001000  0.001000  0.001000   \n",
       "27      0.001000  0.925317  0.066953  0.007486  0.001000  0.001000  0.001000   \n",
       "28      0.001000  0.450735  0.538647  0.010565  0.001000  0.001000  0.001000   \n",
       "29      0.001000  0.180890  0.819059  0.001000  0.001000  0.001000  0.001000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "144338  0.005022  0.260690  0.406058  0.128609  0.001025  0.006966  0.184826   \n",
       "144339  0.001000  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000   \n",
       "144340  0.032840  0.347416  0.267714  0.085864  0.009410  0.040524  0.147880   \n",
       "144341  0.001000  0.809849  0.142712  0.047162  0.001000  0.001000  0.001000   \n",
       "144342  0.001000  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000   \n",
       "144343  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000   \n",
       "144344  0.001000  0.776325  0.194695  0.028288  0.001000  0.001000  0.001000   \n",
       "144345  0.004291  0.001000  0.001000  0.002878  0.001000  0.988525  0.001000   \n",
       "144346  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000  0.001000   \n",
       "144347  0.001000  0.336146  0.525665  0.132012  0.001000  0.001000  0.005282   \n",
       "144348  0.001000  0.827664  0.100055  0.070153  0.001000  0.001000  0.001000   \n",
       "144349  0.001000  0.572332  0.397430  0.028070  0.001000  0.001000  0.002038   \n",
       "144350  0.001357  0.001000  0.001000  0.001000  0.001000  0.997659  0.001000   \n",
       "144351  0.001000  0.664792  0.293183  0.042009  0.001000  0.001000  0.001000   \n",
       "144352  0.001000  0.903193  0.092301  0.001000  0.001000  0.001000  0.001000   \n",
       "144353  0.001000  0.001000  0.001000  0.001000  0.001000  0.044817  0.001000   \n",
       "144354  0.072238  0.001000  0.001000  0.001677  0.001000  0.737002  0.162776   \n",
       "144355  0.001000  0.001000  0.001000  0.001000  0.001000  0.094825  0.001000   \n",
       "144356  0.442786  0.001000  0.001000  0.001000  0.001000  0.001000  0.001000   \n",
       "144357  0.001000  0.001000  0.001000  0.001000  0.999000  0.001000  0.001000   \n",
       "144358  0.007232  0.001000  0.001000  0.002481  0.001000  0.966677  0.001000   \n",
       "144359  0.001000  0.872030  0.096009  0.026228  0.001000  0.001000  0.002657   \n",
       "144360  0.001000  0.001000  0.001000  0.001000  0.001000  0.038284  0.961553   \n",
       "144361  0.001000  0.531110  0.413039  0.054133  0.001000  0.001000  0.001378   \n",
       "144362  0.001000  0.027134  0.357569  0.509527  0.001000  0.001000  0.105559   \n",
       "144363  0.384997  0.001606  0.001000  0.005161  0.001000  0.557151  0.021771   \n",
       "144364  0.001402  0.325014  0.496746  0.136171  0.001000  0.001000  0.040175   \n",
       "144365  0.001000  0.671773  0.312076  0.015415  0.001000  0.001000  0.001000   \n",
       "144366  0.001000  0.466384  0.059653  0.473535  0.001000  0.001000  0.001000   \n",
       "144367  0.001000  0.382231  0.519363  0.058214  0.001000  0.001000  0.039444   \n",
       "\n",
       "         class_8   class_9      id  \n",
       "0       0.001000  0.001000       1  \n",
       "1       0.321769  0.003071       2  \n",
       "2       0.001000  0.001000       3  \n",
       "3       0.001000  0.001000       4  \n",
       "4       0.001525  0.928398       5  \n",
       "5       0.001000  0.001000       6  \n",
       "6       0.999000  0.001000       7  \n",
       "7       0.001000  0.001000       8  \n",
       "8       0.001000  0.001000       9  \n",
       "9       0.215042  0.006446      10  \n",
       "10      0.001000  0.001118      11  \n",
       "11      0.001000  0.939708      12  \n",
       "12      0.001000  0.001000      13  \n",
       "13      0.001000  0.001000      14  \n",
       "14      0.001000  0.008453      15  \n",
       "15      0.137492  0.001000      16  \n",
       "16      0.001000  0.001000      17  \n",
       "17      0.001000  0.001000      18  \n",
       "18      0.001000  0.001000      19  \n",
       "19      0.001000  0.001000      20  \n",
       "20      0.001000  0.001000      21  \n",
       "21      0.999000  0.001000      22  \n",
       "22      0.001474  0.034144      23  \n",
       "23      0.958579  0.015429      24  \n",
       "24      0.010292  0.808263      25  \n",
       "25      0.999000  0.001000      26  \n",
       "26      0.001000  0.001000      27  \n",
       "27      0.001000  0.001000      28  \n",
       "28      0.001000  0.001000      29  \n",
       "29      0.001000  0.001000      30  \n",
       "...          ...       ...     ...  \n",
       "144338  0.002917  0.003886  144339  \n",
       "144339  0.001000  0.001000  144340  \n",
       "144340  0.047727  0.020626  144341  \n",
       "144341  0.001000  0.001000  144342  \n",
       "144342  0.001000  0.001000  144343  \n",
       "144343  0.999000  0.001000  144344  \n",
       "144344  0.001000  0.001000  144345  \n",
       "144345  0.001442  0.002731  144346  \n",
       "144346  0.001000  0.001000  144347  \n",
       "144347  0.001000  0.001000  144348  \n",
       "144348  0.001000  0.001226  144349  \n",
       "144349  0.001000  0.001000  144350  \n",
       "144350  0.001000  0.001000  144351  \n",
       "144351  0.001000  0.001000  144352  \n",
       "144352  0.002898  0.001000  144353  \n",
       "144353  0.001000  0.954635  144354  \n",
       "144354  0.001942  0.023210  144355  \n",
       "144355  0.905166  0.001000  144356  \n",
       "144356  0.556996  0.001000  144357  \n",
       "144357  0.001000  0.001000  144358  \n",
       "144358  0.004247  0.018775  144359  \n",
       "144359  0.001000  0.001522  144360  \n",
       "144360  0.001000  0.001000  144361  \n",
       "144361  0.001000  0.001000  144362  \n",
       "144362  0.001000  0.001000  144363  \n",
       "144363  0.010221  0.018949  144364  \n",
       "144364  0.001000  0.001000  144365  \n",
       "144365  0.001000  0.001000  144366  \n",
       "144366  0.001000  0.001000  144367  \n",
       "144367  0.001000  0.001000  144368  \n",
       "\n",
       "[144368 rows x 10 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49502 samples, validate on 12376 samples\n",
      "Epoch 1/10\n",
      "49502/49502 [==============================] - 12s - loss: 0.9291 - val_loss: 0.6100\n",
      "Epoch 2/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.5873 - val_loss: 0.5631\n",
      "Epoch 3/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.5590 - val_loss: 0.5341\n",
      "Epoch 4/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.5394 - val_loss: 0.5259\n",
      "Epoch 5/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.5226 - val_loss: 0.5203\n",
      "Epoch 6/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.4520 - val_loss: 0.4678\n",
      "Epoch 7/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.4273 - val_loss: 0.4671\n",
      "Epoch 8/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.4157 - val_loss: 0.4602\n",
      "Epoch 9/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.4012 - val_loss: 0.4592\n",
      "Epoch 10/10\n",
      "49502/49502 [==============================] - 11s - loss: 0.3990 - val_loss: 0.4592\n",
      "11808/12376 [===========================>..] - ETA: 0s[[ 231   10    2    0    1   28   14   41   83]\n",
      " [   1 2698  431   42    5    3   25   10    7]\n",
      " [   1  675  805   30    1    1   29    7    1]\n",
      " [   1  160   81  285    5   12   11    2    0]\n",
      " [   0    8    1    0  524    0    0    2    0]\n",
      " [  14   13    1    6    0 2695   32   25   24]\n",
      " [   8   57   35    7    0   28  382   18    3]\n",
      " [  25    8    3    0    2   39   23 1568   27]\n",
      " [  37   15    1    2    0   19    7   28  950]]\n",
      "classification report results:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.56      0.63       410\n",
      "          1       0.74      0.84      0.79      3222\n",
      "          2       0.59      0.52      0.55      1550\n",
      "          3       0.77      0.51      0.61       557\n",
      "          4       0.97      0.98      0.98       535\n",
      "          5       0.95      0.96      0.96      2810\n",
      "          6       0.73      0.71      0.72       538\n",
      "          7       0.92      0.93      0.92      1695\n",
      "          8       0.87      0.90      0.88      1059\n",
      "\n",
      "avg / total       0.82      0.82      0.82     12376\n",
      "\n",
      "log-loss for classifier: 0.4592365163666902\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD8CAYAAACrbmW5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns5lZBf0f5sOkCc5MayUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb083134da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(1500,input_shape=(X_train.shape[1],),activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(750,activation='relu'))\n",
    "model1.add(Dense(9,activation='softmax'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "model1.fit(X_train.values,OHE_y_train,validation_data=[X_val.values,OHE_y_val],callbacks=[cb],epochs=10)\n",
    "ann_pred = model1.predict_classes(X_val.values)\n",
    "from sklearn.metrics import classification_report,log_loss\n",
    "print(confusion_matrix(y_pred=ann_pred,y_true=y_val))\n",
    "sns.heatmap(confusion_matrix(y_pred=ann_pred+1,y_true=y_val),cmap='Greens',xticklabels=range(1,10),yticklabels=range(1,10))\n",
    "print('classification report results:\\r\\n' + classification_report(y_pred=ann_pred,y_true=y_val))\n",
    "print('log-loss for classifier: {}'.format(log_loss(y_pred=model1.predict(X_val.values),y_true=y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-loss for classifier: 0.4583844790366684\n"
     ]
    }
   ],
   "source": [
    "print('log-loss for classifier: {}'.format(log_loss(y_pred=np.clip(model1.predict(X_val.values),a_min=0.0001,a_max=0.9999),y_true=y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = np.clip(,a_min=0.001,a_max=0.999)\n",
    "subm = pd.DataFrame(test_pred)\n",
    "subm.columns = ['class_'+ str(x) for x in range(1,10)]\n",
    "subm['id'] = te_ids\n",
    "subm.to_csv('../subm/ANN_0.4583.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
