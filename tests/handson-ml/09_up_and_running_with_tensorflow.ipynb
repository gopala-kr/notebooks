{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 9 â€“ Up and running with TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 9._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.74651413e+01],\n",
       "       [  4.35734153e-01],\n",
       "       [  9.33829229e-03],\n",
       "       [ -1.06622010e-01],\n",
       "       [  6.44106984e-01],\n",
       "       [ -4.25131839e-06],\n",
       "       [ -3.77322501e-03],\n",
       "       [ -4.26648885e-01],\n",
       "       [ -4.40514028e-01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with pure NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.69419202e+01]\n",
      " [  4.36693293e-01]\n",
      " [  9.43577803e-03]\n",
      " [ -1.07322041e-01]\n",
      " [  6.45065694e-01]\n",
      " [ -3.97638942e-06]\n",
      " [ -3.78654265e-03]\n",
      " [ -4.21314378e-01]\n",
      " [ -4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+00   6.60969987e-17   5.50808322e-18   6.60969987e-17\n",
      "  -1.06030602e-16  -1.10161664e-17   3.44255201e-18  -1.07958431e-15\n",
      "  -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ..., -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.111111111111\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393812],\n",
       "       [-0.04269557],\n",
       "       [-0.66145277],\n",
       "       [-0.63752776]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above except for the `gradients = ...` line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you find the partial derivatives of the following function with regards to `a` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754914"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.212537\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.06855798]\n",
      " [ 0.82962859]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.30571091]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                # not shown\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145277],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. You can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") # not shown in the book\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 9\\n          }\\n        }\\n        tensor_content: &quot;<stripped 743040 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 82560 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mse_grad/Reshape&quot;\\n  input: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape&quot;\\n  input: &quot;gradients/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/mse_grad/Shape_1&quot;\\n  input: &quot;gradients/mse_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/mse_grad/Prod_1&quot;\\n  input: &quot;gradients/mse_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/mse_grad/Prod&quot;\\n  input: &quot;gradients/mse_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/mse_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mse_grad/Tile&quot;\\n  input: &quot;gradients/mse_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square_grad/mul/x&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mse_grad/truediv&quot;\\n  input: &quot;gradients/Square_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Sum&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                     # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.07033372]\n",
      " [ 0.86371452]\n",
      " [ 0.12255151]\n",
      " [-0.31211874]\n",
      " [ 0.38510373]\n",
      " [ 0.00434168]\n",
      " [-0.01232954]\n",
      " [-0.83376896]\n",
      " [-0.80304712]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                             # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      # not shown\n",
    "        return tf.maximum(z, 0., name=\"max\")                          # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a `threshold` variable the classic way, by defining it outside of the `relu()` function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Do' b'you' b'want' b'some' b'caf\\xc3\\xa9?']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some cafÃ©?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Home-Made Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "df_dx = Mul(Const(2), Mul(x, y))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(x, x), Const(1))  # df/dy = xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.000400000048216\n",
      "df/dy(3,4) = 10.000000000047748\n"
     ]
    }
   ],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func.evaluate()\n",
    "    for var in vars_list:\n",
    "        original_value = var.value\n",
    "        var.value = var.value + eps\n",
    "        tweaked_func_eval = func.evaluate()\n",
    "        var.value = original_value\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives\n",
    "\n",
    "df_dx, df_dy = gradients(f, [x, y])\n",
    "print(\"df/dx(3,4) =\", df_dx)\n",
    "print(\"df/dy(3,4) =\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.0\n",
      "df/dy(3,4) = 10.0\n"
     ]
    }
   ],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "\n",
    "x = Var(3.0, name=\"x\")\n",
    "y = Var(4.0, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "df_dx = f.derive(x)  # 2xy\n",
    "df_dy = f.derive(y)  # xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation (autodiff) â€“ forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}Îµ\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 + 4.0Îµ"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4Îµ)\\times(5 + 7Îµ) = 3 \\times 5 + 3 \\times 7Îµ + 4Îµ \\times 5 + 4Îµ \\times 7Îµ = 15 + 21Îµ + 20Îµ + 28Îµ^2 = 15 + 41Îµ + 28 \\times 0 = 15 + 41Îµ$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0 + 41.0Îµ"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + Îµ\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "df_dx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + Îµ\n",
    "\n",
    "df_dy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n",
      "df_dx = 24.0\n",
      "df_dy = 10.0\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)\n",
    "\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", result)\n",
    "print(\"df_dx =\", x.gradient)\n",
    "print(\"df_dy =\", y.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Logistic Regression with Mini-Batch Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9633a33a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($x_0 = 1$) to every instance. For this, we just need to add a column full of 1s on the left of the input matrix $\\mathbf{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05146968,  0.44419863],\n",
       "       [ 1.        ,  1.03201691, -0.41974116],\n",
       "       [ 1.        ,  0.86789186, -0.25482711],\n",
       "       [ 1.        ,  0.288851  , -0.44866862],\n",
       "       [ 1.        , -0.83343911,  0.53505665]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's reshape `y_train` to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's create a small function to generate training batches. In this implementation we will just pick random instances from the training set for each batch. This means that a single batch may contain the same instance multiple times, and also a single epoch may not cover all the training instances (in fact it will generally cover only about two thirds of the instances). However, in practice this is not an issue and it simplifies the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.93189866,  0.13158788],\n",
       "       [ 1.        ,  1.07172763,  0.13482039],\n",
       "       [ 1.        , -1.01148674, -0.04686381],\n",
       "       [ 1.        ,  0.02201868,  0.19079139],\n",
       "       [ 1.        , -0.98941204,  0.02473116]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that the data is ready to be fed to the model, we need to build that model. Let's start with a simple implementation, then we will add all the bells and whistles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _moons_ dataset has two input features, since each instance is a point on a plane (i.e., 2-Dimensional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Logistic Regression model. As we saw in chapter 4, this model first computes a weighted sum of the inputs (just like the Linear Regression model), and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "\n",
    "$\\hat{p} = h_\\mathbf{\\theta}(\\mathbf{x}) = \\sigma(\\mathbf{\\theta}^T \\cdot \\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\mathbf{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, \\dots, \\theta_n$. The input vector $\\mathbf{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "Since we want to be able to make predictions for multiple instances at a time, we will use an input matrix $\\mathbf{X}$ rather than a single input vector. The $i^{th}$ row will contain the transpose of the $i^{th}$ input vector $(\\mathbf{x}^{(i)})^T$. It is then possible to estimate the probability that each instance belongs to the positive class using the following equation:\n",
    "\n",
    "$ \\hat{\\mathbf{p}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{\\theta})$\n",
    "\n",
    "That's all we need to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = 1 / (1 + tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in chapter 4, the log loss is a good cost function to use for Logistic Regression:\n",
    "\n",
    "$J(\\mathbf{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "One option is to implement it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7  # to avoid an overflow when computing the log\n",
    "loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (1 - y) * tf.log(1 - y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might as well use TensorFlow's `tf.losses.log_loss()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.log_loss(y, y_proba)  # uses epsilon = 1e-7 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty standard: let's create the optimizer and tell it to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now (in this minimal version) is the variable initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the model and use it for predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really nothing special about this code, it's virtually the same as the one we used earlier for Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.792602\n",
      "Epoch: 100 \tLoss: 0.343463\n",
      "Epoch: 200 \tLoss: 0.30754\n",
      "Epoch: 300 \tLoss: 0.292889\n",
      "Epoch: 400 \tLoss: 0.285336\n",
      "Epoch: 500 \tLoss: 0.280478\n",
      "Epoch: 600 \tLoss: 0.278083\n",
      "Epoch: 700 \tLoss: 0.276154\n",
      "Epoch: 800 \tLoss: 0.27552\n",
      "Epoch: 900 \tLoss: 0.274912\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't use the epoch number when generating batches, so we could just have a single `for` loop rather than 2 nested `for` loops, but it's convenient to think of training time in terms of number of epochs (i.e., roughly the number of times the algorithm went through the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance in the test set, `y_proba_val` contains the estimated probability that it belongs to the positive class, according to the model. For example, here are the first 5 estimated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54895616],\n",
       "       [ 0.70724374],\n",
       "       [ 0.51900256],\n",
       "       [ 0.9911136 ],\n",
       "       [ 0.50859052]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can go for maximum likelihood: classify as positive any instance whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]], dtype=bool)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, you may want to choose a different threshold than 0.5: make it higher if you want high precision (but lower recall), and make it lower if you want high recall (but lower precision). See chapter 3 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the model's precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86274509803921573"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88888888888888884"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD/CAYAAADi+OGRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJztnX2UFOWV8H+XYcYZQAiMvHGzhiGe1WMccEZB44aAnpAYja/rVzZBR8WzEszMumZ3c0zw4AeGJd+bmORVFOMnzLrkzWJMFI2JURdEc8QPRNy8mKNiCGMyjjo6wwDDcN8/qnvo6amqruqu7q7qvr9z6sx01VPVt56peW7de597H1FVDMMwDCMXY8otgGEYhpEMTGEYhmEYgTCFYRiGYQTCFIZhGIYRCFMYhmEYRiBMYRiGYRiBMIVhGIZhBMIUhmEYhhEIUxiGYRhGIMaWW4AoOeyww3T69OnlFsMwDCMxPPvss2+p6tQgbStKYUyfPp3NmzeXWwzDMIzEICI7grY1l5RhGIYRCFMYhmEYRiBMYRiGYRiBqKgYhmEYlc3g4CA7d+5kz5495RYlcdTX13PEEUdQW1ub9zVMYRiGkRh27tzJoYceyvTp0xGRcouTGFSVnp4edu7cyUc+8pG8r2MuKaMy6OqCU06BN98styRGEdmzZw+NjY2mLEIiIjQ2NhZsmZnCMCqD5cth40bnp1HRmLLIjyj6zRSGkXy6uuDOO+HAAednECvDLBLDCI0pDCP5LF/uKAuAoaFgVoZZJEYe1NTU0NrayowZM/j7v/97du/eHfoaixYt4uWXXwbgG9/4xohjH//4xyORs1iYwjCSTdq62LfP+bxvH9xyC7z4Yu5zwlgkRiLp3NrJ9BunM+aGMUy/cTqdWzsLul5DQwMvvPACL730EnV1ddxyyy2hr/GTn/yEY489FhitMDZt2lSQfMXGFIaRbDKtizQHDsCFFwY7J6hFYiSOzq2dLP7lYnb07kBRdvTuYPEvFxesNNLMnTuXP/zhDwB8//vfZ8aMGcyYMYMbb7wRgP7+fs4880xaWlqYMWMGa9euBeDUU09l8+bNLFmyhIGBAVpbW2lrawNgwoQJACxYsIAHH3xw+LsuvfRSfvaznzE0NMRVV13FiSeeyHHHHcett94ayb0ExRSGkWyeeuqgdZHJyy8ftBwy4xVuFolZGRXJ0keXsntwpMto9+Bulj66tOBr79+/n4ceeoiZM2fy7LPPcuedd/K73/2Op59+mttuu43nn3+ehx9+mA996ENs2bKFl156idNPP33ENb71rW8NWyydnSOV2Be+8AV++tOfArBv3z4effRRzjzzTG6//XYmTZrEM888wzPPPMNtt93Ga6+9VvD9BMUUhpFsnn8eVJ2tvR3q6pz9tbWwZImjKK6++mC8ws0iMSujInmj941Q+4OQtghmz57NtGnTuOyyy9i4cSPnnnsu48ePZ8KECZx33nls2LCBmTNn8utf/5qvfe1rbNiwgUmTJgX+njPOOIPHHnuMvXv38tBDDzFv3jwaGhp45JFHuOeee2htbeVjH/sYPT09vPLKK3nfT1gscc+oDNwshzVrHGXw5JMH4xVHHjnaItm3D9x8x11dsGABrF0Lhx9e/HswImXapGns6B1diHXapGl5XzNtEQTh6KOP5rnnnmP9+vVcc801zJ8/n+uuuy7QufX19Zx66qn86le/Yu3atSxYsABwEvB+/OMf85nPfCbveygEszCMysDLcsj+ecopBy2SzO35592vaTOpEsuK+SsYVztuxL5xteNYMX9FpN8zd+5cfv7zn7N79276+/u57777mDt3Lrt27WLcuHFcdNFFXHXVVTz33HOjzq2trWVwcND1ul/4whe488472bBhw7A76zOf+QwrV64cPmf79u309/dHej9+mMIwKgOvWEYmYeIVNpMq8bTNbGPVWatomtSEIDRNamLVWatom9kW6feccMIJXHrppZx00kl87GMfY9GiRRx//PFs3bqVk046idbWVm644QauueaaUecuXryY4447bjjonclpp53GE088wac+9SnqUq7WRYsWceyxx3LCCScwY8YMLr/8cvbv3x/p/fiiqhWzzZo1S40Mdu1SnTdPtaur3JIUl+z73LVLtb7ezY5QratT7ejIfc32dtXaWuec2tpg5xhF5+WXXy63CInGrf+AzRpwjI3UwhCRK0Rks4jsFZG7crT9FxF5U0TeE5E7ROSQjGPTReQxEdktIr8XkU9FKWfVUC0ulez7dHNPpfGKV2SSti7SroLBQbMyDIPoXVK7gH8D7vBrJCKfAZYA84Em4Ejghowm9wLPA43AUuBnIhJozVkjRbW4VNzu08s91drqHa/IZPnyg3GPNPv3V77iNYwcRKowVHWdqv4c6MnRdCFwu6puU9V3gOXApQAicjRwAnC9qg6o6n8BW4Hzo5S14gmbnJa02kppea++evR9Zk61zRXYduOppw5aF2kGB3NbJoZR4ZQr6N0MbMn4vAX4oIg0po69qqrvZx1vLqF8ySaf5LRyuK8KUVLLl8OGDc7U2aiT8Navh/r6kfsaGuCuu5KlVA0jYsqlMCYAvRmf078f6nIsffxQtwuJyOJU3GRzd3d35IImkrDJadlunS1bSjMw5quk0vKqjnYdRZGE59V/bW3VERMyDA/KpTD6gIkZn9O/v+9yLH38fVxQ1VWqOltVZ0+damEOwN2H7xfszXZflWJgLCTGUmhQOxde/ffyy5UfEzIMH8qlMLYBLRmfW4A/q2pP6tiRInJo1vFtJZQv2YTx4bu5r7ZtK/7AmG8BwGx5wXEXdXWFj1V44dZ/7e1OuZGw8hoVhYjwla98Zfjz9773PZYtWxb598S17HnU02rHikg9UAPUiEi9iLiVH7kHuExEjhWRDwDXAHcBqOp24AXg+tT55wLHAf8VpaxGCr+39WINjIUUAFyyBPbudZezWIF7K1iYbCJ8Lg455BDWrVvHW2+9FYFg3sS17HnUFsY1wADOlNmLUr9fIyLTRKRPRKYBqOrDwHeAx4A3gB3A9RnXWQDMBt4BvgV8TlUtQFEM/DKkvQbGQv8BCykA+OCDzht/tpybNhUvcB9lwcKkzUarBCJ8LsaOHcvixYv5wQ9+MOpYd3c3559/PieeeCInnngiTz755PD+T3/60zQ3N7No0SKampqGFc4555zDrFmzaG5uZtWqVQDxLnseNMMvCZtlehdIe7uTCZ0rM7q9XXXMmPyzn1tb3bOwW1v9z8vM4G5oGJnB7nesUPKV141C+67KCZ3pHfFzMX78eO3t7dWmpiZ999139bvf/a5ef/31qqp6wQUX6IYNG1RVdceOHXrMMceoquo//uM/6je+8Q1VVX3ooYcU0O7ublVV7enpUVXV3bt3a3Nzs7711lvD35P9vaqq69at00suuURVVffu3atHHHGE7t69W2+99VZdvny5qqru2bNHZ82apa+++uoo+WOV6W0knCDB8igSAt1iBLt2wcSJuaf+esU9irko0vPPOzGMMWOgoyO/WElXF5x8cnUkU8aJIjwXEydO5JJLLuFHP/rRiP2/+c1vuOKKK2htbeXv/u7veO+99+jr62Pjxo3D1WZPP/10Jk+ePHzOj370I1paWjj55JP54x//mLNUednLngfVLEnYzMIoAZlWSNC6TEGv6/fm7VYfKv3G6HcsCqJ4S21vd84fM8a97zLrYVVLDbA8CGVhFOG5SL/p9/T0aFNTky5btmzYwmhsbNSBgYFR57S0tIx42588ebJ2d3frY489pnPmzNH+/n5VVT3llFP0scceG/E92d+rqnrxxRfr/fffrxdccIHef//9qqp63nnn6cMPP5xTfrMwjNLhFfwtNG8jiNXiF0co9qJIfm+pQWISXV1wR6paTvo62fGhTD97tdQAKzZFfC6mTJnC5z//eW6//fbhfaeddho//vGPhz+n182YM2fO8Op5jzzyCO+88w4Avb29TJ48mXHjxvH73/+ep59+evjc2JY9D6pZkrCZhVFkvGIczc2F+eWDWC1+cYQoYwzZ5HpLDRKTSLfxqpyb+R319aqHHBK9lVQhhLIwivBcZL7pv/nmm9rQ0DBsYXR3d+vnP/95nTlzpn70ox/Vyy+/XFVV//znP+snP/lJbW5u1kWLFunhhx+ue/bs0T179ujpp5+uxxxzjJ599tkjLIyvfvWreswxx+iFF1446nv37dunkydP1ksvvXR439DQkF599dU6Y8YMbW5u1lNPPVXffffdUfIXamGUfZCPcjOFkYNdu1Q/9jHVk0/ObyDy+gcUyX+AK7Y7qVD8JgIEcVX5lVpPD16Z3zFmjLfbykhkefM9e/bo4OCgqqpu2rRJW1payiaLuaSM4CxfDr/7HTz9dH5meTES2uK+xrbfRIAgAVW3+6urOxg8X79+pJvvwAFvt5WRSN544w1OPPFEWlpauPLKK7ntttvKLVL+BNUsSdjMwvBh166Dro6066PQt/gorINiupOKSdB7z3V/bhZMtjWzcKEFwVMk0cKIE2ZhGMFYvnxkye69e+GEEwp7e43COii0FHm5CHrvue4v19Ky+/bBAw8cDIJb4h/OGGeEJYp+M4VRDaRn6WQOcKrO/iVL8r9u2CKHlURU9+6lUNLbrl3Q339wBtnVV1f1DKr6+np6enpMaYREVenp6aE+u2x/SKSSOn727Nm6efPmcosRPzo64NZb3WtG1dTAzp1w+OGll6tSSPfvl74EN90U/bVvv91RRnV1jhUzNOQUXHz11ar7uw0ODrJz50727NlTblESR319PUcccQS16ZhjChF5VlVnB7mGKYxq4PjjITUn3JWOjugHujjT1QULFsDatYUPuF1dcOSRsGdP9IN45rWzqauDRYuq6+9mFIUwCsNcUnGhmL7pTLfHrl2jV5NLz8SpFv94lIlxxSxJkmvdD5tBZZQYUxhxoVTZvX7lwashwzifWlheirTYZc9zBcTjNP3YqApMYcSBKAr6BcWrPPgTT1RHYbx8LAIvRVrsHJLMooeNjaOPV8sEAyM2mMKIA8Vwa7i9FXd1OTNuYPQqdfPmFc+1EhfysQj8lHmxZ4llfvfu3SP/Xult/Xr3v3M1uBaN0hM0YSMJWyIT9wpNfvOqbOpW48irZlPcy3NERdD1PrzOKXWpjszvFnES+NzauP2dbc0NIyBYLakEkc8gln1+9uDgVuPITykUKkNSCJtVXk5F6vbdNTW5F40q5kJSRkUSRmFEvab3FBG5T0T6RWSHiFzo0e6h1JKt6W2fiGzNOP66iAxkHH8kSjljRSFuDS93iZuLy8/fXi0JeGGzystZ58rruzMTLXP9nSvVtWiUj6CaJcgG3AusBSYAnwB6geYA5z0OXJfx+XXgU2G/P5EWRiG4uUu83oqbm8O9XRvlrXPl9d2HHeYcd/s719dXh2vRiBRCWBhjo1I8IjIeOB+Yoap9wEYR+QVwMeBZf0JEpgNzgUujkqUq8ArgpstIZDI05ARBX3qp9HImmXLWs3r+effEvf5+x5J0s0DcpuAODTk1w557ruqywo3oidIldTSwX1W3Z+zbAjTnOO8SYIOqvp61v1NEukXkERFp8TpZRBaLyGYR2dzd3Z2X4InEy2XxwAPV4V6qBsK6ETNLo6fZt89RPOaaMiIgSoUxAXgva18vcGiO8y4B7sra1wZMB5qAx4BficgH3E5W1VWqOltVZ0+dOjWszMnFK+7w4Q8ns/qrMRq/2FKuooWaldVfybk1RsmIUmH0AROz9k0E3vc6QUQ+ARwO/Cxzv6o+qaoDqrpbVb8JvIvjtjLSJLUsuBGcQv/GFgA3IiZKhbEdGCsiR2XsawG2+ZyzEFiXinn4oYAUKF/lYolaRjbFLltiVCWRKQxV7QfWAV8XkfEiMgc4G1jt1l5EGoDPk+WOEpFpIjJHROpEpF5ErgIOA56MStaKoxpqQBnBSL88XH11vJe+NRJJ1KVBOoAG4C84U2zbVXWbiMwVkWwr4hwcV9NjWfsPBVYC7wB/Ak4HzlDVnohlrQxKWYfKiD/pl4cHH7TJD0bk2HoYSSd7gR1bI6F6KebaHEbFYuthVAvmpzYyKUWQ2+JlVY0pjCRTztIVRrwo1cuDxcuqGlMYScDrra5aakAZuSnFy4PFy6oeUxhJwOutLr3ATl2d87muzolpWC5G9VGKlwfL66h6LOgdd/wCmW61hizYWV10dcGCBbB2bbC/edj2mefZs1aRWNC7kvB7q7MYhhE2ppBvDMLvWbNAeNVgCiPO5ApkWgyjugkbUygkBuH3rFkgvGowhRFXurpg1ix/C8LqSVU3YWMKhcQgvJ619estEF5FmMKIK2lT3ywIww0/69PNRVSsabcWCK8qTGHEkfQ/NziBxa4up1T1vHnO72ZBGH4xBTcXUTHiXZY4WnWYwogjXms1u/mJLeBYnXjFFJ54wt1F5BeDyPcZskkXVYcpjLjh9tZ2xx3efmILOIanEpSsV0xh3ryRLxsnnODc5/r1BxdTStPQAA89lP8zZJMuqg7Lw4gbmcUE04xJ6fUDB0YWGLRic/nR0QG33goXXwyvvRY+JyGuuOVKAFx6qfN8ZD9XdXVwwQXO/dszVLVYHkaSybVWc6af2AKO4cmcWrpmDWzYUDn95uYiAli9Gv77v92tgQcesGfICIwpjHwopksj29WQWfojzdAQLFliAcd8yFayqpXTb24vG+Dc5ymnjF7v++SToa9v9DO0ZUvyXXZGUTCFkQ9RB6D9zvPyE2e+GaaxN0R/suNDaSql39IvG7t2jY5XuMW+nn4aBgdHthsagrY2i4sZ7qhqZBswBbgP6Ad2ABd6tFsGDAJ9GduRGcdbgWeB3amfrUG+f9asWVp0du1Sra933tMaGlS7ug4ea29XHTNGtaMj2HXmzXPOzzwvc78fra1uIU9nv+FOe7tqXZ17v2X/LZOM233W1R18LjOfYbdNpPL6xPAE2KxBx/igDQNdzFmWdS0wAfgE0As0u7RbBqzxuEZdStn8C3AIcGXqc12u7y+Jwsj8Z/T6Jwzyj5ZWEgsXjjxv4cLgSifzOkHbVzNeSjb7b5l0cr1MeD3DuY4ZFUlZFAYwHtgHHJ2xbzXwLZe2fgrjNJy1vCVj3xvA6blkKLrCcHszSyuHMP9omdepqVGtrXV+r611PgdVOmGVlOFQzdaZ3zPsdyzotYNYx0asCKMwooxhHA3sV9XtGfu2AM0e7c8SkbdFZJuItGfsbwZeTN1Imhd9rlM6vBKVwgagswOvaT/y4KDzOb2/mLWBqplqrsGVK0O8kLiY5QRVPFEqjAnAe1n7eoFDXdr+FPgoMBX4InCdiFyQcZ3egNdBRBaLyGYR2dzd3Z2v7MGIIgDtFXjNJpfSsbIMRj74JdsVkohnq/FVBVEqjD5gYta+icD72Q1V9WVV3aWqQ6q6Cfgh8Lmw10lda5WqzlbV2VOnTi3oBnLi9Wb64Q8H/0fzmivvht/bnZVlMPLBz7pav/5gvbKwlpdZu1VBlApjOzBWRI7K2NcCbAtwrgKS+n0bcJyISMbx4wJepzyEcXF4zZXPngYJ/m93VpYhPJVQEqSY5OtSMmu3aohMYahqP7AO+LqIjBeROcDZOIHvEYjI2SIyWRxOwpkJdX/q8OPAEHCliBwiIlek9v82KlnLipdyGRgI51evRj98oQO++di9KcSlZNZu1RB14l4H0AD8BWeKbbuqbhORuSLSl9FuAfAHHDfTPcC3VfVuAFXdB5wDXAK8C/wDcE5qf+Vhb73BKWTANx+7P4W4lMzaDU3n1k6m3zidMTeMYfqN0+nc2llukQJhxQfLTboQ3pe+5BQUNNwptNBiZlHHzAKOhnvRQitEWDQ6t3ay+JeL2T24e3jfuNpxrDprFW0z20oujxUfTAr21hucQt6Azcfuj7mUSsrSR5eOUBYAuwd3s/TRpWWSKDimMMqJzSwJRqEDvg2I/phLqaS80fuG6/4dvTti75oyhVEu7K03OIUO+DYg+pOeQNHe7qy90tFR+RMoysi0SdM8jy3+5eJYKw1TGOXC3nqDU+iAX40zysJi7tGS8TdT/sbzWNxdU6YwyoW99QbHBvziY+7RktC5tZPfvuafIbCjd0dsZ0/ZLCnDqHaCzpLq6oIFCypnSdsyMP3G6ezo3RG4vSAoStOkJlbMX1GUWVQ2S8owjOAEdY+65MEkNZ+gXHgFvL1QnBf6Hb07YhHfMIUBljxnVDdB3KMuMY50PsGO3h0oGptBLc74BbxzEYf4hikMsJIRRnUTJEbkEuNIcj5BuVgxfwXjaseN2CcI7bPbaZrUlPP8sBZK1JjCsNkhlYtZjr4Edid5TAHfu9PdF1/uQS3OtM1sY9VZq2ia1IQgNE1qYvV5q7n5zJtZMX8FtWNqfc8vxEKJAlMYNjukcjHL0ZNQ7iSPGMe3n57geu1yD2pxp21mG6//8+scuP4Ar//z6yMC2SOLdI9kXO04VsxfUQoRPaluhWHJc5WLWY6+hHInecQ4/vdbjaPcK3EY1JLK0keXsm/IvcZq06SmstWayqS6FYYlz1UuZjn64uU2ytw/7LI6ZwvTf9BE54trRsQ4pvz+9VHulTgMaknF628iyChLpFyMLbcAZcWS5yoTL8vx2mstfyDFtEnTXPMB0u6k7IqqaZcVMGLgapvZFouBrBLI9TeJA9VtYVgGcWVilmNO3GbrZLqTbAZU/uSbm5LrbxIHqlthGJWJWY45cZutk+lOCuKyMkZTSG5Krr9JHLDSIMXCyigYCcarhEXTpCZe/+fXSy9QQkhiv5WtNIiITBGR+0SkX0R2iMiFHu2uEpGXROR9EXlNRK7KOv66iAyISF9qeyRKOUuCTek0EkwS3CNxpNIts6hdUjcB+4APAm3AShFpdmknOGt2TwZOB64QkQVZbc5S1Qmp7bSI5SwuNqXTSDhp90hjQ+PwvoaxDaGvU221prwC1HEKXBdCZApDRMYD5wPXqmqfqm4EfgFcnN1WVb+jqs+p6n5V/X/A/cCcqGQpOzal06gQBvYPDP/eM9ATqlZUNdaaqnTLLEoL42hgv6puz9i3BXCzMIYRJ7VxLrAt61CniHSLyCMi0uJz/mIR2Swim7u7u/OVPTosGdCoEAqdKVWNM62SELguhCgVxgTgvax9vcChOc5blpLjzox9bcB0oAl4DPiViHzA7WRVXaWqs1V19tSpU/MQO2KimNJpNZCMGFCoP77S/fle+JX+SDpRKow+YGLWvonA+14niMgVOLGMM1V1b3q/qj6pqgOqultVvwm8i2OFxJ8opnRawNyIAYX64yvdn1+NRKkwtgNjReSojH0tjHY1ASAi/wAsAear6s4c11acQHn8Wb8e5s1zrIT2dhgzBjo6gicDWsDciAmF+uMr3Z9fjUSmMFS1H1gHfF1ExovIHOBsYHV2WxFpA74BfFpVX806Nk1E5ohInYjUp6bcHgY8GZWsRSVtHSxZkt/AbwHz/AjqxjN3X2AK9cdXuj+/KlHVyDZgCvBzoB94A7gwtX8u0JfR7jVgEMeNld5uSR1rBl5MXaMHeBSYHeT7Z82apWVl1y7V+nqnwEhNjWptrfN7XZ1qR0e489NbQ4NqV1fxZU867e2qY8b49/OuXap/9VeqIsH+HkYkrHlxjTb9oEllmWjTD5p0zYtryi2SkQGwWYOO8UEbJmEru8Job3eUg1uFKhHVLVvCnx9U2VQzmYrWT8Fecokp4pAUOtiveXGNjlsxTlnG8DZuxThTGjEijMKwWlJRkT2dNhtVuNA18f0gVgMpHGn30tVX53bjdXVBZ8b8//37zd2Xg1x5FEGS8qpxam0lY7WkoqKjA26/3VthAIjArl1WWyoqOjrglluciQVDQwf3NzTAq6+O7OeFC+Gee0ae79bOGMavLtKK+StGlD8HJ6CdHaMYc8MYlNFjjCAcuP7AqP1G6SlbLamqxs06SFNbe/CnvdVGwwsvOMpCdaSygNHWQ7Z14dXOGIFfHkVQy8Gm1lYWpjCiwm1tjaOPdo4NDjo/01nfW7bYTJ1Cuegip4/dGBwc6cZbvny0UnFrZ4zAb7APmpRnU2srC1MYxeLXv4bt20fvHxqCtjZLzCuEF16AbSPTe3bXwEBN6vex8F/fXzR87O3frne/TmurLZblg99gP6Vhius52fttam1lYTGMYjFlCrzzjvsxEeft2Hzo+TFjxiiFsR9AYKzCnhr46ckTuGTj+6OWGgV3X7vhTufWTpY+upQ3et9g2qRprJi/graZbRz2ncPoGegZ1b6xoZG3vvpWGSQ18sViGOXm1792VxaPPupkf6djGpaYF56uLnj55VG7x+IoC4D6Ifjc7/rgzTdtlk6ReHvgbdf9PQM9VVHGvFoxhVEMvvAF9/3nneddydYykIOxfPlBhZtiP7A/q3BMjTpt/Xzt1bZWQ1j8ptX6Ba2roYx5tWIKI2q6urxdUb293pVsreBgMFxmo2VaF2kOGQI2bfIc2KY0TKm6tRrC4meducU33NoZlYUpjKhZvhzq6ryPuyXmPfGEFRwMSsZstM4X1zB+xThkGcPbmGVCxwPtTpvnn/cM3ALmqsqBn3WWGcwOe76RXExhRI1fPgY4M3Oyp9/Om2cFB/PAbQbO6vNWc/OZN/u2WXXWKk8fvA1yB8mVQ5Fe98FLaViuReVhCiNq0m/A7e0HLY26OicrOfXWOwJboa8ggixWk90GYIy4P/o2yB0kaA6F5VrkRxJjaKYwikEYJRDFCn1GYNKB3CEdnciXPcgl8R86SoLmUFiuRXiSut655WEUA7e6UnV1sGgR3HTTyLbHH+8komVjSWVFwas+Uo3UcPe5dw8Pcpa/YRQTvzpdaSu4VITJwzCFUQxMCcQWr2J44PyzphPU+vb1WWKaUTTiVJTREvfKjVtdKbf4hVFyvGIUgoxwD7gpC3AS0+LuNjDiT1RFGUvtNo1UYYjIFBG5T0T6RWSHiLguACEO3xaRntT2bRGRjOOtIvKsiOxO/WyNUk6jenEL0AriaXW4YVNvjUKJYqJAOeIgUVsYNwH7gA8CbcBKEWl2abcYOAdoAY4DzgIuBxCROuB+YA0wGbgbuD+13zCUVGquAAAVWUlEQVQKIjtA29jQGEpZAK6+52qj2icEFIrXRAEgcL+Wo+xNZDEMERkPvAPMUNXtqX2rgT+p6pKstpuAu1R1VerzZcAXVfVkETkNuBM4IrV8ICLyBrBYVR/2kyE2MQwjEbgFtoNQIzXsv25/kaSKPzYhoDiE7deo4iDlimEcDexPK4sUWwA3C6M5dcytXTPwoo7UZC96XMcw8sbtDS2NX9kLtym51YQVdCwOYfu1HItTRakwJgDvZe3rBQ71aNub1W5CKo6RfczvOojIYhHZLCKbu7u78xLcqE78srr9yl74lcOoBoIunmSEI2y/liNhMkqF0QdMzNo3EXg/QNuJQF/KqghzHVR1larOVtXZU6dOzUtwozrxehNrmtRE28w2y2D2wJZdLQ5h+7UcCZNRKoztwFgROSpjXwuwzaXtttQxt3bbgOMyZ03hBMbdrmMYeZNLIYT5h6ymILAp0uKQT78GKY0TKaoa2Qb8J3AvMB6Yg+NKanZp9yXgf4C/Bj6Eowy+lDpWB+wAvgwcAlyR+lyX6/tnzZqlhrHmxTXa9IMmlWWiTT9o0jUvromkrd81xq0YpyxjeBu3Ylxe10oKUfSbMZpy9CuwWQOO8ZFmeovIFOAO4NNAD7BEVf9DROYCD6nqhFQ7Ab4NpBde/gnwtZTwiMjxqX3HphTLZaqaM+vNZkkZ5ZjBE6cyD4YRFisNEgVdXbBgAaxda2tuJ4hyDN5xKvNgGGGx0iBRYCvgJZJyzOCxILBRLZjCcCNdntxWwEsc5Ri8LQhsVAumMNzIXKPCbW2Kri44+WT42781ZRIzyjF423oQRrVgMYxsurrgyCNhz56D+xoa4NVXD8YyOjpg5cqDv2evcWGUlc6tnSx9dOlwqfIV81fY4G0YHljQuxByLX7U1QUf+Qjs3escq6+H116zwLhhGL7E9UXGgt6F8NRTI5UFOJ83bXJ+X74cBgdHHrPAeNWT9MS9pMsfd5K6JGs2ZmGEIdu6SGNWRlWT9OqtSZe/lORrJcQ5V8csjGKRbV2kMSujqkl69daky18qCrESKqVgoymMMDz11MHZU5kcOHDQZWVUHUkfDLzk3NG7w9xTGRSiWL2mdSuaqD42hREGr7W6bb3uqibpiXt+cibV1x6WIDGcQl4M3KZ7p0lSH5vCMIwCyTf3Iy6BZr/BDJLlnsqnT4O6mrwU6xgZk/N7MnN13EhKH5vCMIwCySdxL06zZnINZpAM91q+fRrU1eSlWId0KND3pEuRC+J6PAl9bLOkDKMMxHXWTFzlCkJY2dMzntzOAffikZ1bO1l430LXZXqD9lHc+thmSRlGzIlroDzJdbHC9GmmNeKFmwuqbWYbB9S9AnHQv12S+9gUhmFETBA/elwD5UmuixWmT93cUJn4DeCF/u2S3MfmkjKMCAmaBGfJctETpk+91jABxzXkl5BXaX87c0kZRpkIGkBN8ltmXAnTp17WQDqO4Pd3qOa/XSQWRmpp1tuB04C3gKtV9T882l4FLASaUm1vVtXvZhx/HfggkI4qbVLV04LIYRaGUW5s9b1kEIWVENdigmEph4VxE7APZ6BvA1aKSLOXfMAlwGTgdOAKEVmQ1eYsVZ2Q2gIpC8OIA3GNTRgjKdRKCDKFNy55NlFSsIUhIuOBd4AZqro9tW818CdVXRLg/B+l5Pin1OfXgUWq+puwspiFYZSbSvNvG+7kmhqbpOeg1BbG0cD+tLJIsQXwsjCGEREB5gLbsg51iki3iDwiIi05rrFYRDaLyObu7u6wshtGpOSbxFdpb6JJI+zfINcU3kot6BiFhTEX+L+qenjGvi8Cbap6ao5zbwDOAU5S1b2pfXOA53BcV19Obceo6ru5ZDELw0gaSXoTrTQyE/cEGRF7qpEaFOWAHqBGalg8azE3n3nz8PFcFkaSYlmRWhgi8riIqMe2EegDJmadNhF4P8d1r8CJZZyZVhYAqvqkqg6o6m5V/SbwLo4VYhixJ+ybaqW+icad7MS97MF9SIeGE/SGdIiVm1fS8WDH8PFcyXd+daeSbEnmVBiqeqqqisf2CWA7MFZEjso4rYXRbqZhROQfgCXAfFXdmUsE8Ci+YhglJJcyyKeWUVwzviudXIl7bqx6dtXw77lcj351p8pdO6wQCo5hqGo/sA74uoiMT7mUzgZWu7UXkTbgG8CnVfXVrGPTRGSOiNSJSH1qCu5hwJOFymkYhRBEGeRjLZRqVpXFSUaSj0LOrh+VLiZ44PoDo3I3shVKjdSMul4SLcmoptV2AA3AX4B7gXZV3QZOjENE+jLa/hvQCDwjIn2p7ZbUsUOBlTizrv6EM+32DFXtiUhOw8iLIMogH2vBy7Xx2aM+G9kAH6fKuG6UQ5nlo5C9qsx6kalQCq0/FRciURiq+raqnqOq41V1WmbSnqpuUNUJGZ8/oqq1GXkWE1T1S6lj21T1uNR1GlV1vqpaFNsoO0GUQT7WgptrY2HLQu7ecndkA3y54iRBFEG5lFmuNUDcGF83Pu/vq5T8HCsNYhgBCPIP/9mjPuvaxmt/mmzXxvpX1kc6wJcjThJUEZRLmWUr6saGRupq6nzP6d/Xn/f3JblCbSamMAwjAEH+4de/st71XK/9XkQ9wAdRdlG7hYIqgnIG/TMV9YS6Cewb2ufb3q0fg0yEmH7jdC5edzENYxtobGhMdP0pUxiGEYAgCXm5Br+gg3Kh7ovs7/nsUZ/1VXbFcAsFVQRh77VY8Y5cCsrNGsjVb9nHewZ6GNg/wOrzVucscBhXTGEYRkD8ZsWA/+AXZlAuxH3h9j13b7mbhS0LPZVdMdxCQRVBmHstZrzDTxl7WQO5+q0Sc2xMYRhGRPgNfmEGj0IK43l9z/pX1nsqu2K4hYIqgjD3WswB2EveNeet8bQGcvVbJebYjC23AIZRKWS+sWeXvL543cWu53gNHm0z2/JyWeQzSE2bNM21zEUhM3j8+sKtbZB79bqHHb076NzaWZCLJ4y8aXL1WzH6tdzYinuGUQJy1R4q9vc0NjQyoW6C62CYlHpWXvcG5ZE3V78lpV9txT3DiBn5xiXCBnndvqeupo739r43wvd/8bqLh2sjJWUFOb/ciXLEBnL1W1L6NQxmYRhGiQi7Qlu+b6jZ39O3r4+egdHFEgRh9XmrEzWAdW7t5KJ1F7kei2Ml2CQQxsIwhWEYMSUqN5ZXqe18rhUHSuXeqxbMJWUYFYBfkDdMDoJfkLUcM3YKzaUolXvPGI0pDMOIKX4DfZgchBXzV3gWzivljJ3OrZ0c9p3DuGjdRQXlUuS7qmGuJDtTJrkxl5RhxBS3GEY2Qd0wHQ92sHLzyhH76mrquOPsO0oSw8h1L1G7k4LGcZomNbFi/opEzGYqFuaSMowKoWFsg+/xoC6lOdPmUDumdsS+Ur4s5lqwKErXmJs14aYs0t9biRnZxcIUhmHEkPSg5zXQpQnqUlr66FIGDwyO2Dd4YJAvP/TlvGUMQy6FEKVrLMxqetMmTavIjOxiYQrDMGJIkEEvu4Cgnw/ea/DrGehBbpCi++39FEIhZb7d7jvoQJ/+3kpZq6IURKYwRGSKiNwnIv0iskNELvRpu0xEBjNW3OsTkSMzjreKyLMisjv1szUqOQ0jCfgNetmB3iBF+XINfsVeuGjF/BWe602k3T9hv9vrvqc0THFt39jQ6Boor5S1KkpBZEFvEbkXRwFdBrQCDwIfTy/VmtV2GfA3qjoqA0dE6oBXgBuBm4HLga8AR6mqb8F6C3oblUKYXIMgbf0S3jKpkRruPvfuogR7D/vOYb4utrCBZr8yKAP7B0IFscMmVVYSJQ96i8h44HzgWlXtU9WNwC8A94pr/pyKUxTxRlXdq6o/AgT4ZBSyGkYSCPPWG8QH3zazjcaGxpzfO6RDRZtu+vbA277Hwwaave777YG3Q0+7zVW63nCIqlrt0cB+Vd2esW8LcIrPOWeJyNtAF/B/VDU9568ZeFFHmj4vpvY/HJG8hhFrwlRPDVoV9Ydn/DDnNF0YOXBntk+7fDLlC4OXnJmECTT73Xe+1X4Nf6KKYUwA3sva1wsc6tH+p8BHganAF4HrROSCjGv1Br2WiCwWkc0isrm7uzsf2Q0jlgR9681n7QnAM5kPijPd1K94YJowgWaLPZSeQApDRB4XEfXYNgJ9wMSs0yYC77tdT1VfVtVdqjqkqpuAHwKfSx0Oe61VqjpbVWdPnTo1yO0YRkURJvM5rYT0emX1eaupkRrXa0Y53dRtXWsYrbDCDvaVWA027gRySanqqX7HUzGMsSJylKq+ktrdAowKeHt9BQw/PduAr4iIZLiljgNuCngtw6g68nHBpNu7ZTmnVwksdAGg7AzvnoGe4ZXsINyCRV73YAqidETiklLVfmAd8HURGS8ic4CzgdVu7UXkbBGZLA4nAVcC96cOPw4MAVeKyCEickVq/2+jkNUwkkSxaxz5vaVH4fLxc2tZoDl5RDmtdgpwB/BpoAdYoqr/kTo2F3hIVSekPt8LnAYcAuwEbk7Nhkpf63jgJ8CxwP8Al6nq87lksGm1RiURhxXbCp1u6lVaPU5rV1TzlFqw9TDKLYZhREIlrPsQ1T0Ua1CPg1IuN1Z80DASSLb7yWsKapJqHEXh1gqSyZ4vVngwHKYwDCMGuA2KcVjDolCimMlUzEHdCg+GI6rEPcMwCsBtUFQUQUbEAOKcZ+DlNip0JlMxB/WgSY+Gg1kYhhEDvAY/RRORZ1BMt1Exq8la8l84TGEYRgzwGvzSweFyTz3NNb3Xy2208L6FBSuNYg7qlvwXDnNJGUYM8FomNA5vutkzidxqSnlZSOlihpltwxKmrla+1zcFEQybVmsYMSGu+QBBpsb6zerKbmvEC5tWaxgJI67KAoIFnXMVFiz3rKNiZ8xXC6YwDKPMFDNgHAVBgs7pWIBXMcMxMqZs9xP3/k0SpjAMo8zEPXksTPn0u8+929XSyF6YqZTEvX+ThCkMwygzcU8eC1s+3cvSKNcgHff+TRI2S8owykwSksfCzCRqm9nGxevcV2cuxyCdhP5NCmZhGEaZqcTksWIm24WlEvu3XJjCMIwyU4nJY3EapCuxf8uF5WEYhlEU4jxV2DiIrYdhGIZhBMIS9wzDMIzIiURhiMgUEblPRPpFZIeIXOjT9iER6cvY9onI1ozjr4vIQMbxR6KQ0TCMeGPZ2PEnqmm1NwH7gA8CrcCDIrJFVbdlN1TVMzI/i8jjwG+zmp2lqr+JSDbDMGJOkAKHRvkp2MIQkfHA+cC1qtqnqhuBXwDuE7FHnjsdmAvcU6gchmEkF8vGTgZRuKSOBvar6vaMfVuA5gDnXgJsUNXXs/Z3iki3iDwiIi1+FxCRxSKyWUQ2d3d3hxLcMIx4YNnYySAKhTEBeC9rXy9waIBzLwHuytrXBkwHmoDHgF+JyAe8LqCqq1R1tqrOnjp1alCZDcOIEXFK9DO8yakwRORxEVGPbSPQB0zMOm0i8H6O634COBz4WeZ+VX1SVQdUdbeqfhN4F8dtZRhGhRKnRD/Dm5xBb1U91e94KoYxVkSOUtVXUrtbgFEB7ywWAutUtS+XCIDkktMwjORS7FX1jGiIJHFPRP4TZ2BfhDNLaj3wcbdZUqn2DcCbwLmq+tuM/dOADwPP4Fg//wR8FThGVXtyyWGJe4ZhGOEoR+JeB9AA/AW4F2hPKwsRmSsi2VbEOTiupsey9h8KrATeAf4EnA6cEURZGIZhGMXFSoMYhmFUMVYaxDAMw4gcUxiGYRhGIExhGIZhGIGoqBiGiHQDo9diLC2HAW+VWYZ8MdnLR5LlT7LskGz5o5C9SVUDZT1XlMKIAyKyOWgAKW6Y7OUjyfInWXZItvyllt1cUoZhGEYgTGEYhmEYgTCFET2ryi1AAZjs5SPJ8idZdki2/CWV3WIYhmEYRiDMwjAMwzACYQrDMAzDCIQpjAIQkStSq/3tFZG7ArT/FxF5U0TeE5E7ROSQEojpJ88UEblPRPpFZIeIXOjTdpmIDIpIX8Z2ZBzlFYdvi0hPavu2iJS1RH4I2cvezy4yBX7OY/iMB5JdRC4VkaGsfj+1dJK6ynSIiNyeel7eF5EXROQMn/ZF73tTGIWxC/g34I5cDUXkM8ASYD7OaoJHAjcUVbrc3ATsAz6Is9LhShHxW1p3rapOyNheLYmUBwkq72KcisgtwHHAWcDlpRLSgzB9Xe5+zibQcx7TZzzw/yjwVFa/P15c0XIyFvgjcAowCbgG+KmITM9uWKq+N4VRAKq6TlV/DgQpv74QuF1Vt6nqO8By4NJiyudHauGr84FrVbVPVTcCvwAuLpdMfoSUdyHw76q6U1X/BPw71td5E+I5j9UzDqH/R2OFqvar6jJVfV1VD6jqA8BrwCyX5iXpe1MYpaMZ2JLxeQvwQRFpLJM8RwP7VXV7lkx+FsZZIvK2iGwTkfbiijeKMPK69bXffRWbsH1dzn4uhLg942E5XkTeEpHtInKtiORckbSUiMgHcZ4lt4XpStL3pjBKxwSgN+Nz+vdDyyALOPK8l7WvF295fgp8FJgKfBG4TkQuKJ54owgjr1tfTyhjHCOM7OXu50KI2zMehv8GZgD/C8cavAC4qqwSZSAitUAncLeq/t6lSUn63hSGByLyuIiox7Yxj0v2ARMzPqd/f79waUcTQP5sedIyucqjqi+r6i5VHVLVTcAPgc8VQ3YPwsjr1td9Wr6ko8Cyx6CfC6Gkz3iUqOqrqvpayvWzFfg6Mel3ERkDrMaJgV3h0awkfW8KwwNVPVVVxWP7RB6X3IYThE3TAvy5WMvPBpB/OzBWRI7Kksl1HXa3rwBK+cYeRl63vg56X8WgkL4udT8XQkmf8SITi35PWcW340yWOF9VBz2alqTvTWEUgIiMFZF6oAaoEZF6H7/nPcBlInKsiHwAZ8bDXSUSdRSq2g+sA74uIuNFZA5wNs6bzChE5GwRmZyasnoScCVwf0zlvQf4VxH5axH5EPAVEtLX5e5nN0I857F6xiG47CJyRipGgIgcA1xLmfs9xUocF+VZqjrg0640fa+qtuW5Actw3kQyt2WpY9NwzMRpGe3/Ffgzjj/7TuCQMss/Bfg50A+8AVyYcWwujhsn/flenJkmfcDvgSvjIq+LrAJ8B3g7tX2HVBmcuPV1HPvZRXbX5zwhz3gg2YHvpeTuB17FcUnVlln2ppS8e1Kypre2cvW91ZIyDMMwAmEuKcMwDCMQpjAMwzCMQJjCMAzDMAJhCsMwDMMIhCkMwzAMIxCmMAzDMIxAmMIwDMMwAmEKwzAMwwiEKQzDMAwjEP8fJW34iKj3NNEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9633917ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty bad, doesn't it? But let's not forget that the Logistic Regression model has a linear decision boundary, so this is actually close to the best we can do with this model (unless we add more features, as we will show in a second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start over, but this time we will add all the bells and whistles, as listed in the exercise:\n",
    "* Define the graph within a `logistic_regression()` function that can be reused easily.\n",
    "* Save checkpoints using a `Saver` at regular intervals during training, and save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will add 4 more features to the inputs: ${x_1}^2$, ${x_2}^2$, ${x_1}^3$ and ${x_2}^3$. This was not part of the exercise, but it will demonstrate how adding features can improve the model. We will do this manually, but you could also add them using `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the \"enhanced\" training set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -5.14696757e-02,   4.44198631e-01,\n",
       "          2.64912752e-03,   1.97312424e-01,  -1.36349734e-04,\n",
       "          8.76459084e-02],\n",
       "       [  1.00000000e+00,   1.03201691e+00,  -4.19741157e-01,\n",
       "          1.06505890e+00,   1.76182639e-01,   1.09915879e+00,\n",
       "         -7.39511049e-02],\n",
       "       [  1.00000000e+00,   8.67891864e-01,  -2.54827114e-01,\n",
       "          7.53236288e-01,   6.49368582e-02,   6.53727646e-01,\n",
       "         -1.65476722e-02],\n",
       "       [  1.00000000e+00,   2.88850997e-01,  -4.48668621e-01,\n",
       "          8.34348982e-02,   2.01303531e-01,   2.41002535e-02,\n",
       "         -9.03185778e-02],\n",
       "       [  1.00000000e+00,  -8.33439108e-01,   5.35056649e-01,\n",
       "          6.94620746e-01,   2.86285618e-01,  -5.78924095e-01,\n",
       "          1.53179024e-01]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the graph, using the `logistic_regression()` function. We will also create the `FileWriter` to save the summaries to the log directory for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train the model! We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example we just save the epoch number to a separate file, but in chapter 11 we will see how to store the training step directly as part of the model, using a non-trainable variable called `global_step` that we pass to the optimizer's `minimize()` method.\n",
    "\n",
    "You can try interrupting training to verify that it does indeed restore the last checkpoint when you start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.629985\n",
      "Epoch: 500 \tLoss: 0.161224\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can make predictions by just classifying as positive all the instances whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's much, much better! Apparently the new features really helped a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try starting the tensorboard server, find the latest run and look at the learning curve (i.e., how the loss evaluated on the test set evolves as a function of the epoch number):\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=tf_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with the hyperparameters (e.g. the `batch_size` or the `learning_rate`) and run training again and again, comparing the learning curves. You can even automate this process by implementing grid search or randomized search. Below is a simple implementation of a randomized search on both the batch size and the learning rate. For the sake of simplicity, the checkpoint mechanism was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(\n",
    "        X, y, learning_rate=learning_rate)\n",
    "\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "    final_model_path = \"./my_logreg_model_%d\" % search_iteration\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        saver.save(sess, final_model_path)\n",
    "\n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. See the exercise solutions for chapter 2 for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing complete; Gopal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
